{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th></tr><tr><td>47</td><td>application_1610641989196_0309</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://resourcemanager.service.consul:8088/proxy/application_1610641989196_0309/\">Link</a></td><td><a target=\"_blank\" href=\"http://jimeuwest-worker-1.internal.cloudapp.net:8042/node/containerlogs/container_e02_1610641989196_0309_01_000001/amlsim__davit000\">Link</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "<pyspark.sql.session.SparkSession object at 0x7faf16a14b50>"
     ]
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a connection to hsfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully."
     ]
    }
   ],
   "source": [
    "import hsfs\n",
    "# Create a connection\n",
    "connection = hsfs.connection()\n",
    "# Get the feature store handle for the project's feature store\n",
    "fs = connection.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve nodes and edges training datasets from hsfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_td = fs.get_training_dataset(\"node_td\", 1)\n",
    "edge_td = fs.get_training_dataset(\"edges_td\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert training dataset in to pandas daframe\n",
    "We are going to use StellarGraph library to compute node embeddings. StellarGraph supports loading data via Pandas DataFrames, NumPy arrays, Neo4j and NetworkX graphs. \n",
    "\n",
    "---\n",
    "**NOTE**:\n",
    "\n",
    "Loading large scale dataset in to StellarGraph for training can not be handled with above mentioned fameworks. It will require loading data using frameworks such as `tf.data`. \n",
    "\n",
    "If your training datasets measure from couple of GB to 100s of GBs or even TBs contact us at Logical Clocks and we will help you to setup distributed training pipelines. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get fg as pandas\n",
    "node_pdf = node_td.read().toPandas()\n",
    "edge_pdf = edge_td.read().drop(\"tran_timestamp\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define hopsworks experiments wrapper function and put all the training logic there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeddings_computer(walk_number, walk_length, emb_size):\n",
    "    \n",
    "    import os\n",
    "    import sys\n",
    "    import uuid\n",
    "    import random    \n",
    "    \n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    import pydoop.hdfs as pydoop\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    from hops import hdfs\n",
    "    from hops import pandas_helper as pandas\n",
    "    from hops import model as hops_model\n",
    "    from hops import tensorboard\n",
    "    \n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn import preprocessing, feature_extraction, model_selection\n",
    "    from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.manifold import TSNE\n",
    "\n",
    "    import stellargraph as sg\n",
    "    from stellargraph import StellarGraph\n",
    "    from stellargraph import StellarDiGraph\n",
    "    from stellargraph.data import BiasedRandomWalk\n",
    "    from stellargraph.data import UnsupervisedSampler\n",
    "    from stellargraph.mapper import Node2VecLinkGenerator, Node2VecNodeGenerator\n",
    "    from stellargraph.layer import Node2Vec, link_classification\n",
    "\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras  \n",
    "        \n",
    "    ###########\n",
    "    batch_size = 128\n",
    "    epochs = 10\n",
    "    num_samples = [20, 20]\n",
    "    layer_sizes = [100, 100]\n",
    "    learning_rate = 1e-2\n",
    "\n",
    "    node_data = pd.DataFrame(node_pdf[['type']], index=node_pdf['id'])\n",
    "    ###########\n",
    "        \n",
    "    print('Defining StellarDiGraph')\n",
    "    G =StellarDiGraph(node_data,\n",
    "                      edges=edge_pdf, \n",
    "                      edge_type_column=\"tx_type\")\n",
    "\n",
    "\n",
    "    nodes = list(G.nodes())\n",
    "\n",
    "    walker = BiasedRandomWalk(\n",
    "        G,\n",
    "        n=walk_number,\n",
    "        length=walk_length,\n",
    "        p=0.5,  # defines probability, 1/p, of returning to source node\n",
    "        q=2.0,  # defines probability, 1/q, for moving to a node away from the source node\n",
    "    )\n",
    "    unsupervised_samples = UnsupervisedSampler(G, nodes=list(G.nodes()), walker=walker)\n",
    "    generator = Node2VecLinkGenerator(G, batch_size)\n",
    "    node2vec = Node2Vec(emb_size, generator=generator)\n",
    "    \n",
    "    x_inp, x_out = node2vec.in_out_tensors()\n",
    "    prediction = link_classification(\n",
    "        output_dim=1, output_act=\"sigmoid\", edge_embedding_method=\"dot\"\n",
    "    )(x_out)\n",
    "\n",
    "    print('Defining the model')\n",
    "    model = keras.Model(inputs=x_inp, outputs=prediction)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(lr=1e-3),\n",
    "        loss=keras.losses.binary_crossentropy,\n",
    "        metrics=[keras.metrics.binary_accuracy],\n",
    "    )\n",
    "    \n",
    "    # Create a callback that saves the model's weights every 5 epochs\n",
    "    cp_callback = keras.callbacks.ModelCheckpoint(\n",
    "        filepath=tensorboard.logdir(), \n",
    "        verbose=1, \n",
    "        save_weights_only=True,\n",
    "        save_freq=5*batch_size\n",
    "    )\n",
    "    \n",
    "    # Save the weights using the `checkpoint_path` format\n",
    "    \n",
    "    print('Training the model')\n",
    "    history = model.fit(\n",
    "        generator.flow(unsupervised_samples),\n",
    "        epochs=epochs,\n",
    "        verbose=0,\n",
    "        use_multiprocessing=False,\n",
    "        workers=4,\n",
    "        shuffle=True,\n",
    "        callbacks=[cp_callback],\n",
    "    )\n",
    "\n",
    "    binary_accuracy = history.history['binary_accuracy'][-1]\n",
    "    metrics={'accuracy': binary_accuracy} \n",
    "    \n",
    "    # save to the model registry\n",
    "    export_path = os.getcwd() + '/model-' + str(uuid.uuid4())\n",
    "    print('Exporting trained model to: {}'.format(export_path))\n",
    "    model.save(export_path)\n",
    "    print('Done exporting!')\n",
    "        \n",
    "    hops_model.export(export_path, 'NodeEmbeddings', metrics=metrics)\n",
    "    \n",
    "    return metrics    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use above experiments wrapper function to conduct hops training experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hops import experiment\n",
    "from hops import hdfs\n",
    "import json\n",
    "args_dict =  {\"walk_number\": [40], \"walk_length\": [5], \"emb_size\": [128]}    \n",
    "experiment.launch(embeddings_computer, args_dict, name='graph_embeddings_compute', metric_key='accuracy', local_logdir=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th></tr><tr><td>30</td><td>None</td><td>pyspark</td><td>idle</td><td></td><td></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "<pyspark.sql.session.SparkSession object at 0x7ff4a70c0e90>"
     ]
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Session 30 unexpectedly reached final status 'dead'. See logs:\n",
      "stdout: \n",
      "2021-02-01 16:44:44,207 WARN  NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2021-02-01 16:44:44,276 WARN  DependencyUtils: Local jar /srv/hops/spark/jars/datanucleus-api.jar does not exist, skipping.\n",
      "2021-02-01 16:44:44,385 INFO  RMProxy: Connecting to ResourceManager at resourcemanager.service.consul/10.0.0.4:8032\n",
      "2021-02-01 16:44:45,160 INFO  Client: Requesting a new application from cluster with 1 NodeManagers\n",
      "2021-02-01 16:44:45,252 INFO  Configuration: found resource resource-types.xml at file:/srv/hops/hadoop-3.2.0.1-EE-RC0/etc/hadoop/resource-types.xml\n",
      "2021-02-01 16:44:45,261 INFO  ResourceUtils: Adding resource type - name = yarn.io/gpu, units = , type = COUNTABLE\n",
      "2021-02-01 16:44:45,278 INFO  Client: Verifying our application has not requested more than the maximum memory capability of the cluster (64000 MB per container)\n",
      "2021-02-01 16:44:45,278 INFO  Client: Will allocate AM container, with 8852 MB memory including 804 MB overhead\n",
      "2021-02-01 16:44:45,278 INFO  Client: Setting up container launch context for our AM\n",
      "2021-02-01 16:44:45,283 INFO  Client: Setting up the launch environment for our AM container\n",
      "2021-02-01 16:44:45,293 INFO  Client: Preparing resources for our AM container\n",
      "2021-02-01 16:44:45,977 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/metrics.properties\n",
      "2021-02-01 16:44:46,063 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/log4j.properties\n",
      "2021-02-01 16:44:46,069 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/hive-site.xml\n",
      "2021-02-01 16:44:46,074 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/Projects/amlsim/Resources/adversarialaml.zip\n",
      "2021-02-01 16:44:46,222 INFO  Client: Uploading resource file:/tmp/spark-bd2d3cfe-ae01-4c4c-9407-11d9b81471e3/__spark_conf__6883521476190275996.zip -> hdfs:/Projects/amlsim/Resources/.sparkStaging/application_1612193978684_0002/__spark_conf__.zip\n",
      "2021-02-01 16:44:46,703 INFO  SecurityManager: Changing view acls to: livy,amlsim__meb10179\n",
      "2021-02-01 16:44:46,704 INFO  SecurityManager: Changing modify acls to: livy,amlsim__meb10179\n",
      "2021-02-01 16:44:46,704 INFO  SecurityManager: Changing view acls groups to: \n",
      "2021-02-01 16:44:46,704 INFO  SecurityManager: Changing modify acls groups to: \n",
      "2021-02-01 16:44:46,705 INFO  SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(livy, amlsim__meb10179); groups with view permissions: Set(); users  with modify permissions: Set(livy, amlsim__meb10179); groups with modify permissions: Set()\n",
      "2021-02-01 16:44:46,757 INFO  EsServiceCredentialProvider: Loaded EsServiceCredentialProvider\n",
      "2021-02-01 16:44:47,637 INFO  EsServiceCredentialProvider: Hadoop Security Enabled = [false]\n",
      "2021-02-01 16:44:47,638 INFO  EsServiceCredentialProvider: ES Auth Method = [SIMPLE]\n",
      "2021-02-01 16:44:47,638 INFO  EsServiceCredentialProvider: Are creds required = [false]\n",
      "2021-02-01 16:44:47,644 INFO  Client: Submitting application application_1612193978684_0002 to ResourceManager\n",
      "2021-02-01 16:44:47,696 INFO  YarnClientImpl: Submitted application application_1612193978684_0002\n",
      "2021-02-01 16:44:47,698 INFO  Client: Application report for application_1612193978684_0002 (state: GENERATING_SECURITY_MATERIAL)\n",
      "2021-02-01 16:44:47,701 INFO  Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1612197887662\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://davitamlgpu-master.internal.cloudapp.net:8088/proxy/application_1612193978684_0002/\n",
      "\t user: amlsim__meb10179\n",
      "2021-02-01 16:44:47,707 INFO  ShutdownHookManager: Shutdown hook called\n",
      "2021-02-01 16:44:47,708 INFO  ShutdownHookManager: Deleting directory /tmp/spark-3cf4a6ca-4f87-493a-b461-b670d5877aa4\n",
      "2021-02-01 16:44:47,712 INFO  ShutdownHookManager: Deleting directory /tmp/spark-bd2d3cfe-ae01-4c4c-9407-11d9b81471e3\n",
      "\n",
      "stderr: \n",
      "\n",
      "YARN Diagnostics: \n",
      "Invalid host name: local host is: (unknown); destination host is: \"resourcemanager.service.consul\":8032; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost\n"
     ]
    }
   ],
   "source": [
    "from hops import hdfs\n",
    "hdfs.copy_to_local(\"hdfs:///Projects/amlsim/Resources/ganAml\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started copying local path ganAml/1 to hdfs path hdfs://rpc.namenode.service.consul:8020/Projects/amlsim/Models/ganAml2/1\n",
      "\n",
      "Finished copying\n",
      "\n",
      "Exported model ganAml2 as version 1 successfully.\n",
      "Polling ganAml2 version 1 for model availability.\n",
      "get model:/hopsworks-api/api/project/119/models/ganAml2_1?filter_by=endpoint_id:119\n",
      "Polling ganAml2 version 1 for model availability.\n",
      "get model:/hopsworks-api/api/project/119/models/ganAml2_1?filter_by=endpoint_id:119\n",
      "Polling ganAml2 version 1 for model availability.\n",
      "get model:/hopsworks-api/api/project/119/models/ganAml2_1?filter_by=endpoint_id:119\n",
      "Polling ganAml2 version 1 for model availability.\n",
      "get model:/hopsworks-api/api/project/119/models/ganAml2_1?filter_by=endpoint_id:119\n",
      "Polling ganAml2 version 1 for model availability.\n",
      "get model:/hopsworks-api/api/project/119/models/ganAml2_1?filter_by=endpoint_id:119\n",
      "Polling ganAml2 version 1 for model availability.\n",
      "get model:/hopsworks-api/api/project/119/models/ganAml2_1?filter_by=endpoint_id:119\n",
      "Polling ganAml2 version 1 for model availability.\n",
      "get model:/hopsworks-api/api/project/119/models/ganAml2_1?filter_by=endpoint_id:119\n",
      "Polling ganAml2 version 1 for model availability.\n",
      "get model:/hopsworks-api/api/project/119/models/ganAml2_1?filter_by=endpoint_id:119\n",
      "Polling ganAml2 version 1 for model availability.\n",
      "get model:/hopsworks-api/api/project/119/models/ganAml2_1?filter_by=endpoint_id:119\n",
      "Polling ganAml2 version 1 for model availability.\n",
      "get model:/hopsworks-api/api/project/119/models/ganAml2_1?filter_by=endpoint_id:119\n",
      "Polling ganAml2 version 1 for model availability.\n",
      "get model:/hopsworks-api/api/project/119/models/ganAml2_1?filter_by=endpoint_id:119\n",
      "Polling ganAml2 version 1 for model availability.\n",
      "get model:/hopsworks-api/api/project/119/models/ganAml2_1?filter_by=endpoint_id:119\n",
      "Polling ganAml2 version 1 for model availability.\n",
      "get model:/hopsworks-api/api/project/119/models/ganAml2_1?filter_by=endpoint_id:119\n",
      "Polling ganAml2 version 1 for model availability.\n",
      "get model:/hopsworks-api/api/project/119/models/ganAml2_1?filter_by=endpoint_id:119\n",
      "Polling ganAml2 version 1 for model availability.\n",
      "get model:/hopsworks-api/api/project/119/models/ganAml2_1?filter_by=endpoint_id:119\n",
      "Polling ganAml2 version 1 for model availability.\n",
      "get model:/hopsworks-api/api/project/119/models/ganAml2_1?filter_by=endpoint_id:119\n",
      "Polling ganAml2 version 1 for model availability.\n",
      "get model:/hopsworks-api/api/project/119/models/ganAml2_1?filter_by=endpoint_id:119\n",
      "Polling ganAml2 version 1 for model availability.\n",
      "get model:/hopsworks-api/api/project/119/models/ganAml2_1?filter_by=endpoint_id:119\n",
      "Polling ganAml2 version 1 for model availability.\n",
      "get model:/hopsworks-api/api/project/119/models/ganAml2_1?filter_by=endpoint_id:119\n",
      "Polling ganAml2 version 1 for model availability.\n",
      "get model:/hopsworks-api/api/project/119/models/ganAml2_1?filter_by=endpoint_id:119\n",
      "Polling ganAml2 version 1 for model availability.\n",
      "get model:/hopsworks-api/api/project/119/models/ganAml2_1?filter_by=endpoint_id:119\n",
      "Polling ganAml2 version 1 for model availability.\n",
      "get model:/hopsworks-api/api/project/119/models/ganAml2_1?filter_by=endpoint_id:119\n",
      "Polling ganAml2 version 1 for model availability.\n",
      "get model:/hopsworks-api/api/project/119/models/ganAml2_1?filter_by=endpoint_id:119\n",
      "Polling ganAml2 version 1 for model availability.\n",
      "get model:/hopsworks-api/api/project/119/models/ganAml2_1?filter_by=endpoint_id:119\n",
      "Model not available during polling, set a higher value for synchronous_timeout to wait longer.\n",
      "'hdfs://rpc.namenode.service.consul:8020/Projects/amlsim/Models/ganAml2/1'"
     ]
    }
   ],
   "source": [
    "from hops import model as hops_model\n",
    "hops_model.export('ganAml', 'ganAml2', metrics={'loss': 0.00033020367845892906 })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
