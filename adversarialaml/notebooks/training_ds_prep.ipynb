{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th></tr><tr><td>48</td><td>application_1612876649427_0002</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://resourcemanager.service.consul:8088/proxy/application_1612876649427_0002/\">Link</a></td><td><a target=\"_blank\" href=\"http://amlsim-worker-1.internal.cloudapp.net:8042/node/containerlogs/container_e06_1612876649427_0002_01_000001/amlsim__meb10179\">Link</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "sc: org.apache.spark.SparkContext = org.apache.spark.SparkContext@26306cd6\n"
     ]
    }
   ],
   "source": [
    "val sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.sql.types.{FloatType, LongType}\n",
      "import org.apache.spark.sql.types.{ArrayType, StructField, StructType}\n",
      "import org.apache.spark.sql.functions.explode\n",
      "import org.apache.spark.sql.expressions.Window\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.types.{FloatType,LongType}\n",
    "import org.apache.spark.sql.types.{ArrayType, StructField, StructType}\n",
    "import org.apache.spark.sql.functions.explode\n",
    "import org.apache.spark.sql.expressions.Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import com.logicalclocks.hsfs._\n"
     ]
    }
   ],
   "source": [
    "import com.logicalclocks.hsfs._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connection: com.logicalclocks.hsfs.HopsworksConnection = com.logicalclocks.hsfs.HopsworksConnection@60a8bcea\n",
      "fs: com.logicalclocks.hsfs.FeatureStore = FeatureStore{id=67, name='amlsim_featurestore', projectId=119, featureGroupApi=com.logicalclocks.hsfs.metadata.FeatureGroupApi@12624486}\n"
     ]
    }
   ],
   "source": [
    "val connection = HopsworksConnection.builder().build();\n",
    "val fs = connection.getFeatureStore();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_all: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [f_19: double, f_13: double ... 364 more fields]\n"
     ]
    }
   ],
   "source": [
    "val df_all = fs.getFeatureGroup(\"simts_features\", 1).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res1: Long = 60000\n"
     ]
    }
   ],
   "source": [
    "df_all.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res2: Long = 1000\n"
     ]
    }
   ],
   "source": [
    "df_all.where($\"target\" === 1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res3: Long = 59000\n"
     ]
    }
   ],
   "source": [
    "df_all.where($\"target\" === 0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WrappedArray(f_19, f_13, f_22, f_23, f_33, f_51, f_58, f_64, f_68, f_72, f_73, f_77, f_83, f_88, f_90, f_92, f_94, f_100, f_103, f_105, f_106, f_107, f_112, f_116, f_117, f_121, f_124, f_125, f_127, f_132, f_141, f_146, f_151, f_158, f_159, f_163, f_165, f_168, f_172, f_178, f_182, f_184, f_190, f_191, f_196, f_204, f_205, f_208, f_214, f_220, f_222, f_223, f_224, f_225, f_228, f_231, f_234, f_237, f_240, f_253, f_257, f_264, f_271, f_272, f_274, f_276, f_279, f_280, f_283, f_286, f_287, f_293, f_303, f_308, f_313, f_316, f_321, f_322, f_324, f_331, f_340, f_342, f_344, f_346, f_352, f_361, f_1, f_5, f_3, f_2, f_6, f_8, f_15, f_34, f_38, f_40, f_42, f_47, f_54, f_65, f_79, f_80, f_81, f_85, f_91, f_93, f_95, f_98, f_102, f_104, f_111, f_113, f_114, f_118, f_120, f_123, f_131, f_133, f_134, f_136, f_144, f_145, f_153, f_154, f_156, f_160, f_164, f_167, f_169, f_173, f_177, f_185, f_186, f_187, f_189, f_194, f_200, f_209, f_211, f_230, f_235, f_242, f_248, f_249, f_250, f_251, f_262, f_263, f_267, f_268, f_277, f_281, f_284, f_285, f_294, f_296, f_297, f_298, f_301, f_305, f_310, f_319, f_320, f_323, f_326, f_328, f_337, f_338, f_348, f_350, f_354, f_357, f_363, f_4, f_12, f_21, f_27, f_28, f_29, f_31, f_32, f_37, f_41, f_44, f_45, f_48, f_50, f_52, f_56, f_57, f_59, f_63, f_66, f_67, f_70, f_71, f_74, f_75, f_76, f_84, f_89, f_96, f_97, f_99, f_110, f_129, f_130, f_135, f_138, f_139, f_142, f_147, f_149, f_152, f_157, f_161, f_162, f_170, f_171, f_176, f_181, f_183, f_192, f_193, f_195, f_197, f_199, f_207, f_212, f_218, f_221, f_227, f_232, f_233, f_238, f_241, f_244, f_245, f_252, f_259, f_261, f_265, f_273, f_275, f_278, f_289, f_290, f_291, f_295, f_300, f_302, f_307, f_312, f_314, f_315, f_317, f_318, f_325, f_329, f_332, f_333, f_334, f_335, f_336, f_339, f_343, f_347, f_349, f_356, f_360, target, f_9, f_11, f_7, f_10, f_14, f_16, f_17, f_18, f_20, f_24, f_25, f_26, f_30, f_35, f_36, f_39, f_43, f_46, f_49, f_53, f_55, f_60, f_61, f_62, f_69, f_78, f_82, f_86, f_87, f_101, f_108, f_109, f_115, f_119, f_122, f_126, f_128, f_137, f_140, f_143, f_148, f_150, f_155, f_166, f_174, f_175, f_179, f_180, f_188, f_198, f_201, f_202, f_203, f_206, f_210, f_213, f_215, f_216, f_217, f_219, f_226, f_229, f_236, f_239, f_243, f_246, f_247, f_254, f_255, f_256, f_258, f_260, f_266, f_269, f_270, f_282, f_288, f_292, f_299, f_304, f_306, f_309, f_311, f_327, f_330, f_341, f_345, f_351, f_353, f_355, f_358, f_359, f_362, f_364, f_365)"
     ]
    }
   ],
   "source": [
    "print(df_all.columns.toSeq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WrappedArray(f_19, f_13, f_22, f_23, f_33, f_51, f_58, f_64, f_68, f_72, f_73, f_77, f_83, f_88, f_90, f_92, f_94, f_100, f_103, f_105, f_106, f_107, f_112, f_116, f_117, f_121, f_124, f_125, f_127, f_132, f_141, f_146, f_151, f_158, f_159, f_163, f_165, f_168, f_172, f_178, f_182, f_184, f_190, f_191, f_196, f_204, f_205, f_208, f_214, f_220, f_222, f_223, f_224, f_225, f_228, f_231, f_234, f_237, f_240, f_253, f_257, f_264, f_271, f_272, f_274, f_276, f_279, f_280, f_283, f_286, f_287, f_293, f_303, f_308, f_313, f_316, f_321, f_322, f_324, f_331, f_340, f_342, f_344, f_346, f_352, f_361, f_1, f_5, f_3, f_2, f_6, f_8, f_15, f_34, f_38, f_40, f_42, f_47, f_54, f_65, f_79, f_80, f_81, f_85, f_91, f_93, f_95, f_98, f_102, f_104, f_111, f_113, f_114, f_118, f_120, f_123, f_131, f_133, f_134, f_136, f_144, f_145, f_153, f_154, f_156, f_160, f_164, f_167, f_169, f_173, f_177, f_185, f_186, f_187, f_189, f_194, f_200, f_209, f_211, f_230, f_235, f_242, f_248, f_249, f_250, f_251, f_262, f_263, f_267, f_268, f_277, f_281, f_284, f_285, f_294, f_296, f_297, f_298, f_301, f_305, f_310, f_319, f_320, f_323, f_326, f_328, f_337, f_338, f_348, f_350, f_354, f_357, f_363, f_4, f_12, f_21, f_27, f_28, f_29, f_31, f_32, f_37, f_41, f_44, f_45, f_48, f_50, f_52, f_56, f_57, f_59, f_63, f_66, f_67, f_70, f_71, f_74, f_75, f_76, f_84, f_89, f_96, f_97, f_99, f_110, f_129, f_130, f_135, f_138, f_139, f_142, f_147, f_149, f_152, f_157, f_161, f_162, f_170, f_171, f_176, f_181, f_183, f_192, f_193, f_195, f_197, f_199, f_207, f_212, f_218, f_221, f_227, f_232, f_233, f_238, f_241, f_244, f_245, f_252, f_259, f_261, f_265, f_273, f_275, f_278, f_289, f_290, f_291, f_295, f_300, f_302, f_307, f_312, f_314, f_315, f_317, f_318, f_325, f_329, f_332, f_333, f_334, f_335, f_336, f_339, f_343, f_347, f_349, f_356, f_360, target, f_9, f_11, f_7, f_10, f_14, f_16, f_17, f_18, f_20, f_24, f_25, f_26, f_30, f_35, f_36, f_39, f_43, f_46, f_49, f_53, f_55, f_60, f_61, f_62, f_69, f_78, f_82, f_86, f_87, f_101, f_108, f_109, f_115, f_119, f_122, f_126, f_128, f_137, f_140, f_143, f_148, f_150, f_155, f_166, f_174, f_175, f_179, f_180, f_188, f_198, f_201, f_202, f_203, f_206, f_210, f_213, f_215, f_216, f_217, f_219, f_226, f_229, f_236, f_239, f_243, f_246, f_247, f_254, f_255, f_256, f_258, f_260, f_266, f_269, f_270, f_282, f_288, f_292, f_299, f_304, f_306, f_309, f_311, f_327, f_330, f_341, f_345, f_351, f_353, f_355, f_358, f_359, f_362, f_364)"
     ]
    }
   ],
   "source": [
    "print(df_all.columns.slice(0,365).map(a => a.toString ).toSeq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.ml.feature.{MinMaxScaler, StandardScaler, Normalizer, MaxAbsScaler}\n",
      "import org.apache.spark.ml.Pipeline\n",
      "import org.apache.spark.ml.feature.VectorAssembler\n",
      "assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_82f3c831cfc8\n",
      "df_all_features: org.apache.spark.sql.DataFrame = [f_19: double, f_13: double ... 365 more fields]\n",
      "mms: org.apache.spark.ml.feature.MinMaxScaler = minMaxScal_8b7dc1535626\n",
      "mms_tr: mms.type = minMaxScal_8b7dc1535626\n",
      "trainingFeaturesPipeline: org.apache.spark.ml.Pipeline = pipeline_ee1dba0c8339\n",
      "trainingFeaturesDF: org.apache.spark.sql.DataFrame = [f_19: double, f_13: double ... 366 more fields]\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.{MinMaxScaler,StandardScaler,Normalizer, MaxAbsScaler}\n",
    "import org.apache.spark.ml.{Pipeline}\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "\n",
    "val assembler = new VectorAssembler().setInputCols(df_all.columns.slice(0,365).map(a => a.toString).toArray).setOutputCol(\"features\")\n",
    "\n",
    "val df_all_features = assembler.transform(df_all)\n",
    "\n",
    "// scaler instance with the min(-1) and max(1) \n",
    "val mms = new MinMaxScaler().\n",
    "    setInputCol(\"features\").\n",
    "    setOutputCol(\"mms\").\n",
    "    setMax(1).\n",
    "    setMin(-1)\n",
    "\n",
    "\n",
    "val mms_tr = mms.setInputCol(\"features\").setOutputCol(\"mms\")\n",
    "\n",
    "val trainingFeaturesPipeline = (new Pipeline()\n",
    "  .setStages(Array(mms_tr)))\n",
    "\n",
    "val trainingFeaturesDF = trainingFeaturesPipeline.fit(df_all_features).transform(df_all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.ml.linalg.DenseVector\n",
      "toArr: Any => Array[Double] = <function1>\n",
      "toArrUdf: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,ArrayType(DoubleType,false),None)\n",
      "final_df_all: org.apache.spark.sql.DataFrame = [target: float, mms: array<float>]\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.linalg.DenseVector\n",
    "val toArr: Any => Array[Double] = _.asInstanceOf[DenseVector].toArray\n",
    "val toArrUdf = udf(toArr)\n",
    "\n",
    "val final_df_all = trainingFeaturesDF.select($\"target\",$\"mms\").\n",
    "             withColumn(\"target\",$\"target\".cast(FloatType)).\n",
    "             withColumn(\"mms\",toArrUdf($\"mms\").cast(ArrayType(FloatType)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "van: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [target: float, mms: array<float>]\n",
      "ben: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [target: float, mms: array<float>]\n"
     ]
    }
   ],
   "source": [
    "val van = final_df_all.where($\"target\"===1)\n",
    "val ben = final_df_all.where($\"target\"===0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "benTrainDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [target: float, mms: array<float>]\n",
      "benEvalDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [target: float, mms: array<float>]\n"
     ]
    }
   ],
   "source": [
    "// Now that the data has been prepared, let's split the dataset into a training and test dataframe\n",
    "val Array(benTrainDF, benEvalDF) = final_df_all.randomSplit(Array(0.8, 0.02),seed = 12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res16: Long = 58539\n"
     ]
    }
   ],
   "source": [
    "benTrainDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res17: Long = 1461\n"
     ]
    }
   ],
   "source": [
    "benEvalDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvalDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [target: float, mms: array<float>]\n"
     ]
    }
   ],
   "source": [
    "val EvalDF = benEvalDF.union(van)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res18: Long = 1024\n"
     ]
    }
   ],
   "source": [
    "EvalDF.where($\"target\"===1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res19: Long = 1437\n"
     ]
    }
   ],
   "source": [
    "EvalDF.where($\"target\"===0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ben_sim_ts_td_meta: com.logicalclocks.hsfs.TrainingDataset = com.logicalclocks.hsfs.TrainingDataset@468e109\n"
     ]
    }
   ],
   "source": [
    "val ben_sim_ts_td_meta = (fs.createTrainingDataset()\n",
    "                .name(\"ben_td\")\n",
    "                .description(\"Ben simulated Time Series Training Dataset\")\n",
    "                .dataFormat(DataFormat.TFRECORD)     \n",
    "                .statisticsEnabled(false)          \n",
    "                .version(1)\n",
    "                .build())\n",
    "ben_sim_ts_td_meta.save(benTrainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_sim_ts_td_meta: com.logicalclocks.hsfs.TrainingDataset = com.logicalclocks.hsfs.TrainingDataset@5dc93511\n"
     ]
    }
   ],
   "source": [
    "val eval_sim_ts_td_meta = (fs.createTrainingDataset()\n",
    "                .name(\"eval_td\")\n",
    "                .description(\"Simulated Time Series Training Dataset for evaluation\")\n",
    "                .dataFormat(DataFormat.TFRECORD)     \n",
    "                .statisticsEnabled(false)          \n",
    "                .version(1)\n",
    "                .build())\n",
    "eval_sim_ts_td_meta.save(EvalDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val testDf = EvalDF.where($\"target\"===1)\n",
    "\n",
    "val test_sim_ts_td_meta = (fs.createTrainingDataset()\n",
    "                .name(\"test_td\")\n",
    "                .description(\"Simulated Time Series Training Dataset for testing\")\n",
    "                .dataFormat(DataFormat.TFRECORD)     \n",
    "                .statisticsEnabled(false)          \n",
    "                .version(1)\n",
    "                .build())\n",
    "test_sim_ts_td_meta.save(testDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "scala",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}