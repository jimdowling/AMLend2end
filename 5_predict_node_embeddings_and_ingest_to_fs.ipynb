{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this notebook we will ingest computed node embeddings as feature groups and also prepare training datasets for anomaly detection model training  \n",
    "\n",
    "---\n",
    "**NOTE**: \n",
    "\n",
    "In real life scenarios financial transaction are dynamically evolving graphs. If live Transaction Monitoring System is based on graph or node embeddings then this will require 1st to update the graph and node representations after new transactions arrive. Recomputing entire graph for every newly arrived transaction will lead to unaxeptable delayes and even monitoring system failures. This problem  will be more sever if large amount of updates happen in a short time window.\n",
    "\n",
    "Contact us at Logical Clocks and we will help you to setup end to end graph based deep anomaly detection live Transaction Monitoring Systems. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Model Repository for best node embeddings model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th></tr><tr><td>5</td><td>application_1613204927129_0007</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://resourcemanager.service.consul:8088/proxy/application_1613204927129_0007/\">Link</a></td><td><a target=\"_blank\" href=\"http://jim21westerneurope-worker-1.internal.cloudapp.net:8042/node/containerlogs/container_e02_1613204927129_0007_01_000001/aml__meb10179\">Link</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "from hops import model\n",
    "from hops.model import Metric\n",
    "MODEL_NAME=\"NodeEmbeddings\"\n",
    "EVALUATION_METRIC=\"accuracy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = model.get_best_model(MODEL_NAME, EVALUATION_METRIC, Metric.MAX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'application_1613204927129_0005_1'"
     ]
    }
   ],
   "source": [
    "best_model['experimentId']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model and load wights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras  \n",
    "\n",
    "import pandas as pd\n",
    "from stellargraph import StellarDiGraph\n",
    "from stellargraph.mapper import Node2VecLinkGenerator, Node2VecNodeGenerator\n",
    "from stellargraph.data import UnsupervisedSampler, BiasedRandomWalk\n",
    "from stellargraph.layer import Node2Vec\n",
    "import pydoop.hdfs as pydoop\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import array, coalesce, concat,  col\n",
    "\n",
    "import hsfs\n",
    "from hops import hdfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully."
     ]
    }
   ],
   "source": [
    "# Create a connection\n",
    "connection = hsfs.connection()\n",
    "# Get the feature store handle for the project's feature store\n",
    "fs = connection.get_feature_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_td = fs.get_training_dataset(\"node_td\", 1)\n",
    "edge_td = fs.get_training_dataset(\"edges_td\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get fg as pandas\n",
    "node_pdf = node_td.read().toPandas()\n",
    "edge_pdf = edge_td.read().drop(\"tran_timestamp\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining StellarDiGraph"
     ]
    }
   ],
   "source": [
    "node_data = pd.DataFrame(node_pdf[['type']], index=node_pdf['id'])\n",
    "\n",
    "print('Defining StellarDiGraph')\n",
    "G =StellarDiGraph(node_data,\n",
    "                      edges=edge_pdf, \n",
    "                      edge_type_column=\"tx_type\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hops import experiment\n",
    "from hops import hdfs\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyperparams_path = \"Resources/embeddings_best_hp.json\"\n",
    "best_hyperparams = json.loads(hdfs.load(best_hyperparams_path))\n",
    "args_dict = {}\n",
    "for key in best_hyperparams.keys():\n",
    "    args_dict[key] = [best_hyperparams[key]]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "walk_number = args_dict['walk_number']\n",
    "walk_length = args_dict['walk_length']\n",
    "batch_size = 1\n",
    "emb_size = args_dict['emb_size'][0]\n",
    "# Extracting node embeddings\n",
    "walker = BiasedRandomWalk(\n",
    "        G,\n",
    "        n=walk_number,\n",
    "        length=walk_length,\n",
    "        p=0.5,  # defines probability, 1/p, of returning to source node\n",
    "        q=2.0,  # defines probability, 1/q, for moving to a node away from the source node\n",
    "    )\n",
    "unsupervised_samples = UnsupervisedSampler(G, nodes=list(G.nodes()), walker=walker)\n",
    "generator = Node2VecLinkGenerator(G, batch_size)\n",
    "\n",
    "node2vec = Node2Vec(emb_size, generator=generator)\n",
    "x_inp, x_out = node2vec.in_out_tensors()\n",
    "\n",
    "x_inp_src = x_inp[0]\n",
    "x_out_src = x_out[0]\n",
    "embedding_model = keras.Model(inputs=x_inp_src, outputs=x_out_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = list(G.nodes())\n",
    "node_gen = Node2VecNodeGenerator(G, batch_size).flow(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "pdf = pd.DataFrame(embedding_model.predict(node_gen), index=G.nodes())\n",
    "emb_feature_names = [\"em_\" + str(c)  for c in pdf.columns]\n",
    "pdf.columns = emb_feature_names\n",
    "pdf['id'] = pdf.index\n",
    "node_embeddings_df = spark.createDataFrame(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-------------------+--------------------+-------------------+--------------------+-------------------+--------------------+--------------------+-------------------+-------------------+--------------------+--------------------+------------------+-------------------+-------------------+--------+\n",
      "|               em_0|               em_1|               em_2|                em_3|               em_4|                em_5|               em_6|                em_7|                em_8|               em_9|              em_10|               em_11|               em_12|             em_13|              em_14|              em_15|      id|\n",
      "+-------------------+-------------------+-------------------+--------------------+-------------------+--------------------+-------------------+--------------------+--------------------+-------------------+-------------------+--------------------+--------------------+------------------+-------------------+-------------------+--------+\n",
      "|-0.6743223667144775|0.33088159561157227|-0.2894887924194336| -0.4159972667694092|-0.3294029235839844|  0.6736867427825928| -0.844062328338623|  0.5046820640563965|0.052224159240722656| 0.9284093379974365| -0.624690055847168|-0.08918213844299316| 0.10274863243103027|0.0888373851776123|-0.9493122100830078|-0.8011047840118408|d40c9dfe|\n",
      "|-0.6612794399261475|-0.6516411304473877| 0.7073519229888916|-0.02201199531555...| 0.7589728832244873|-0.18706417083740234|-0.3340327739715576|-0.10419654846191406| 0.12374615669250488|-0.5719611644744873|-0.5456056594848633| 0.19821572303771973|-0.17507123947143555| 0.668036937713623|-0.2605583667755127| 0.8939676284790039|27d0f368|\n",
      "+-------------------+-------------------+-------------------+--------------------+-------------------+--------------------+-------------------+--------------------+--------------------+-------------------+-------------------+--------------------+--------------------+------------------+-------------------+-------------------+--------+\n",
      "only showing top 2 rows"
     ]
    }
   ],
   "source": [
    "node_embeddings_df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a connection to hsfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully."
     ]
    }
   ],
   "source": [
    "import hsfs\n",
    "from hops import hdfs\n",
    "# Create a connection\n",
    "connection = hsfs.connection()\n",
    "# Get the feature store handle for the project's feature store\n",
    "fs = connection.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve nodes training dataset from hsfs and determine whether node was part of the previously known money laundering scheme or not "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+\n",
      "|      id|type|\n",
      "+--------+----+\n",
      "|d40c9dfe|   0|\n",
      "|27d0f368|   1|\n",
      "|3fcbfffd|   0|\n",
      "|c434d818|   1|\n",
      "|97b145d1|   1|\n",
      "+--------+----+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "node_td = fs.get_training_dataset(\"node_td\", 1)\n",
    "node_td.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+-------+--------+-------+------+\n",
      "|  source|  target|tx_type|base_amt|tran_id|is_sar|\n",
      "+--------+--------+-------+--------+-------+------+\n",
      "|3aa9646b|1e46e726|      4|  858.77|    496|     0|\n",
      "|49203bc3|a74d1101|      4|  386.86|   1342|     0|\n",
      "|616d4505|99af2455|      4|  616.43|   1580|     0|\n",
      "|39be1ea2|e7ec7bdb|      4|  146.44|   2866|     0|\n",
      "|e2e0d938|afc399a9|      4|  439.09|   3997|     0|\n",
      "+--------+--------+-------+--------+-------+------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "edges_td = fs.get_training_dataset(\"edges_td\", 1)\n",
    "edges_td.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|      id|is_sar|\n",
      "+--------+------+\n",
      "|33a8ff5b|     1|\n",
      "|43e028ef|     1|\n",
      "|fcf3bbf3|     1|\n",
      "|8b9017b8|     1|\n",
      "|9c187eed|     1|\n",
      "|65636b63|     1|\n",
      "|68c0230d|     1|\n",
      "|550a25ff|     1|\n",
      "|d73e5230|     1|\n",
      "|c0be245b|     1|\n",
      "|cdbd2ed5|     1|\n",
      "|963b978f|     1|\n",
      "|84563a83|     1|\n",
      "|da77c74b|     1|\n",
      "|840701de|     1|\n",
      "|dc37f73b|     1|\n",
      "|b0f4351c|     1|\n",
      "|dd2ebcf1|     1|\n",
      "|c29d75dc|     1|\n",
      "|d7c99aa5|     1|\n",
      "+--------+------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "alert_edges = edges_td.read().where(F.col(\"is_sar\")==1)\n",
    "alert_sources = alert_edges.select([\"source\"]).toDF(\"id\")\n",
    "alert_targets = alert_edges.select([\"target\"]).toDF(\"id\")\n",
    "alert_nodes = alert_sources.union(alert_targets).dropDuplicates(subset=[\"id\"])\n",
    "alert_nodes = alert_nodes.withColumn(\"is_sar\",F.lit(1))\n",
    "alert_nodes.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|      id|is_sar|\n",
      "+--------+------+\n",
      "|01fdc089|     0|\n",
      "|1a14903a|     0|\n",
      "|243b1e8b|     0|\n",
      "|26c56102|     0|\n",
      "|2906ef08|     0|\n",
      "|33a8ff5b|     1|\n",
      "|3406706a|     0|\n",
      "|3406d993|     0|\n",
      "|43e028ef|     1|\n",
      "|4b46d80d|     0|\n",
      "|5132aa4d|     0|\n",
      "|5628bd6c|     0|\n",
      "|5645140a|     0|\n",
      "|5a99160f|     0|\n",
      "|5c01ec6e|     0|\n",
      "|62827917|     0|\n",
      "|68b90958|     0|\n",
      "|7138cbc6|     0|\n",
      "|8b9017b8|     1|\n",
      "|8c094b0d|     0|\n",
      "+--------+------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "node_embeddings_df = node_embeddings_df.join(alert_nodes,['id'],\"left\")\n",
    "node_embeddings_df = node_embeddings_df.withColumn(\"is_sar\",F.when(F.col(\"is_sar\") == 1, F.col(\"is_sar\")).otherwise(0))\n",
    "node_embeddings_df.select(\"id\",\"is_sar\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7347"
     ]
    }
   ],
   "source": [
    "node_embeddings_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_td = node_embeddings_df.drop(\"id\").withColumn(\"embedding\", array(emb_feature_names)).select(\"is_sar\",\"embedding\").withColumnRenamed(\"is_sar\",\"target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|target|           embedding|\n",
      "+------+--------------------+\n",
      "|     0|[0.22783374786376...|\n",
      "|     0|[0.05608654022216...|\n",
      "|     0|[0.86463785171508...|\n",
      "|     0|[0.32764077186584...|\n",
      "|     0|[-0.5083436965942...|\n",
      "|     1|[-0.9576399326324...|\n",
      "|     0|[0.18058371543884...|\n",
      "|     0|[-0.6948006153106...|\n",
      "|     1|[0.80894732475280...|\n",
      "|     0|[0.87883257865905...|\n",
      "|     0|[0.14162421226501...|\n",
      "|     0|[0.31747889518737...|\n",
      "|     0|[0.39318180084228...|\n",
      "|     0|[0.11511349678039...|\n",
      "|     0|[-0.2011146545410...|\n",
      "|     0|[0.02693414688110...|\n",
      "|     0|[0.88547921180725...|\n",
      "|     0|[-0.3807508945465...|\n",
      "|     1|[-2.6106834411621...|\n",
      "|     0|[-0.2701852321624...|\n",
      "+------+--------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "emb_td.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare training datasets for anomaly detection \n",
    "###### In the next notebook we are going to train [gan for anomaly detection](https://arxiv.org/pdf/1905.11034.pdf). Durring training step  we will provide only features of accounts that have never been reported for money laundering behaviour.  But we will disclose previously reported accounts to the model only in evaluation step.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_sar_df = emb_td.where(col(\"target\")==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sar_df = emb_td.where(col(\"target\")==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that the data has been prepared, let's split the dataset into a training and test dataframe\n",
    "[non_sar_train_df, non_sar_eval_df] = non_sar_df.randomSplit([0.8, 0.02],seed = 12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<hsfs.training_dataset.TrainingDataset object at 0x7f6625b0e550>"
     ]
    }
   ],
   "source": [
    "non_sar_td = fs.create_training_dataset(name=\"gan_non_sar_training_df\",\n",
    "                                       version=1,\n",
    "                                       data_format=\"tfrecord\",\n",
    "                                       label=[\"target\"], \n",
    "                                       statistics_config=False, \n",
    "                                       description=\"non sar dataset for gan training\")\n",
    "non_sar_td.save(non_sar_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = non_sar_eval_df.union(sar_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<hsfs.training_dataset.TrainingDataset object at 0x7f66253de990>"
     ]
    }
   ],
   "source": [
    "gan_eval_ds = fs.create_training_dataset(name=\"gan_eval_df\",\n",
    "                                       version=1,\n",
    "                                       data_format=\"tfrecord\",\n",
    "                                       label=[\"target\"], \n",
    "                                       statistics_config=False, \n",
    "                                       description=\"evaluation dataset for gan training\")\n",
    "gan_eval_ds.save(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
