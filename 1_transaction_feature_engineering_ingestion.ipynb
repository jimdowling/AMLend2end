{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tSession 8 unexpectedly reached final status 'dead'. See logs:\n",
      "stdout: \n",
      "2021-02-13 13:51:15,839 WARN  NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2021-02-13 13:51:15,960 WARN  DependencyUtils: Local jar /srv/hops/spark/jars/datanucleus-api.jar does not exist, skipping.\n",
      "2021-02-13 13:51:16,101 INFO  RMProxy: Connecting to ResourceManager at resourcemanager.service.consul/10.0.0.4:8032\n",
      "2021-02-13 13:51:16,890 INFO  Client: Requesting a new application from cluster with 1 NodeManagers\n",
      "2021-02-13 13:51:16,981 INFO  Configuration: found resource resource-types.xml at file:/srv/hops/hadoop-3.2.0.1-EE-RC0/etc/hadoop/resource-types.xml\n",
      "2021-02-13 13:51:16,990 INFO  ResourceUtils: Adding resource type - name = yarn.io/gpu, units = , type = COUNTABLE\n",
      "2021-02-13 13:51:17,007 INFO  Client: Verifying our application has not requested more than the maximum memory capability of the cluster (24271 MB per container)\n",
      "2021-02-13 13:51:17,007 INFO  Client: Will allocate AM container, with 2432 MB memory including 384 MB overhead\n",
      "2021-02-13 13:51:17,008 INFO  Client: Setting up container launch context for our AM\n",
      "2021-02-13 13:51:17,013 INFO  Client: Setting up the launch environment for our AM container\n",
      "2021-02-13 13:51:17,023 INFO  Client: Preparing resources for our AM container\n",
      "2021-02-13 13:51:17,786 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/metrics.properties\n",
      "2021-02-13 13:51:17,863 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/log4j.properties\n",
      "2021-02-13 13:51:17,869 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/hive-site.xml\n",
      "2021-02-13 13:51:18,016 INFO  Client: Uploading resource file:/tmp/spark-4a519839-362d-435d-a164-260306d4d5cb/__spark_conf__5467427819598746872.zip -> hdfs:/Projects/blah/Resources/.sparkStaging/application_1613224209876_0002/__spark_conf__.zip\n",
      "2021-02-13 13:51:18,478 INFO  SecurityManager: Changing view acls to: livy,blah__meb10179\n",
      "2021-02-13 13:51:18,478 INFO  SecurityManager: Changing modify acls to: livy,blah__meb10179\n",
      "2021-02-13 13:51:18,479 INFO  SecurityManager: Changing view acls groups to: \n",
      "2021-02-13 13:51:18,479 INFO  SecurityManager: Changing modify acls groups to: \n",
      "2021-02-13 13:51:18,480 INFO  SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(livy, blah__meb10179); groups with view permissions: Set(); users  with modify permissions: Set(livy, blah__meb10179); groups with modify permissions: Set()\n",
      "2021-02-13 13:51:18,538 INFO  EsServiceCredentialProvider: Loaded EsServiceCredentialProvider\n",
      "2021-02-13 13:51:19,471 INFO  EsServiceCredentialProvider: Hadoop Security Enabled = [false]\n",
      "2021-02-13 13:51:19,471 INFO  EsServiceCredentialProvider: ES Auth Method = [SIMPLE]\n",
      "2021-02-13 13:51:19,471 INFO  EsServiceCredentialProvider: Are creds required = [false]\n",
      "2021-02-13 13:51:19,478 INFO  Client: Submitting application application_1613224209876_0002 to ResourceManager\n",
      "2021-02-13 13:51:19,542 INFO  YarnClientImpl: Submitted application application_1613224209876_0002\n",
      "2021-02-13 13:51:19,547 INFO  Client: Application report for application_1613224209876_0002 (state: GENERATING_SECURITY_MATERIAL)\n",
      "2021-02-13 13:51:19,551 INFO  Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1613224279498\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://resourcemanager.service.consul:8088/proxy/application_1613224209876_0002/\n",
      "\t user: blah__meb10179\n",
      "2021-02-13 13:51:19,556 INFO  ShutdownHookManager: Shutdown hook called\n",
      "2021-02-13 13:51:19,557 INFO  ShutdownHookManager: Deleting directory /tmp/spark-4a519839-362d-435d-a164-260306d4d5cb\n",
      "2021-02-13 13:51:19,570 INFO  ShutdownHookManager: Deleting directory /tmp/spark-910cc854-850d-484f-933a-a7065a8eff15\n",
      "\n",
      "stderr: \n",
      "\n",
      "YARN Diagnostics: \n",
      "Invalid host name: local host is: (unknown); destination host is: \"resourcemanager.service.consul\":8032; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost.\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tSession 8 unexpectedly reached final status 'dead'. See logs:\n",
      "stdout: \n",
      "2021-02-13 13:51:15,839 WARN  NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2021-02-13 13:51:15,960 WARN  DependencyUtils: Local jar /srv/hops/spark/jars/datanucleus-api.jar does not exist, skipping.\n",
      "2021-02-13 13:51:16,101 INFO  RMProxy: Connecting to ResourceManager at resourcemanager.service.consul/10.0.0.4:8032\n",
      "2021-02-13 13:51:16,890 INFO  Client: Requesting a new application from cluster with 1 NodeManagers\n",
      "2021-02-13 13:51:16,981 INFO  Configuration: found resource resource-types.xml at file:/srv/hops/hadoop-3.2.0.1-EE-RC0/etc/hadoop/resource-types.xml\n",
      "2021-02-13 13:51:16,990 INFO  ResourceUtils: Adding resource type - name = yarn.io/gpu, units = , type = COUNTABLE\n",
      "2021-02-13 13:51:17,007 INFO  Client: Verifying our application has not requested more than the maximum memory capability of the cluster (24271 MB per container)\n",
      "2021-02-13 13:51:17,007 INFO  Client: Will allocate AM container, with 2432 MB memory including 384 MB overhead\n",
      "2021-02-13 13:51:17,008 INFO  Client: Setting up container launch context for our AM\n",
      "2021-02-13 13:51:17,013 INFO  Client: Setting up the launch environment for our AM container\n",
      "2021-02-13 13:51:17,023 INFO  Client: Preparing resources for our AM container\n",
      "2021-02-13 13:51:17,786 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/metrics.properties\n",
      "2021-02-13 13:51:17,863 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/log4j.properties\n",
      "2021-02-13 13:51:17,869 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/hive-site.xml\n",
      "2021-02-13 13:51:18,016 INFO  Client: Uploading resource file:/tmp/spark-4a519839-362d-435d-a164-260306d4d5cb/__spark_conf__5467427819598746872.zip -> hdfs:/Projects/blah/Resources/.sparkStaging/application_1613224209876_0002/__spark_conf__.zip\n",
      "2021-02-13 13:51:18,478 INFO  SecurityManager: Changing view acls to: livy,blah__meb10179\n",
      "2021-02-13 13:51:18,478 INFO  SecurityManager: Changing modify acls to: livy,blah__meb10179\n",
      "2021-02-13 13:51:18,479 INFO  SecurityManager: Changing view acls groups to: \n",
      "2021-02-13 13:51:18,479 INFO  SecurityManager: Changing modify acls groups to: \n",
      "2021-02-13 13:51:18,480 INFO  SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(livy, blah__meb10179); groups with view permissions: Set(); users  with modify permissions: Set(livy, blah__meb10179); groups with modify permissions: Set()\n",
      "2021-02-13 13:51:18,538 INFO  EsServiceCredentialProvider: Loaded EsServiceCredentialProvider\n",
      "2021-02-13 13:51:19,471 INFO  EsServiceCredentialProvider: Hadoop Security Enabled = [false]\n",
      "2021-02-13 13:51:19,471 INFO  EsServiceCredentialProvider: ES Auth Method = [SIMPLE]\n",
      "2021-02-13 13:51:19,471 INFO  EsServiceCredentialProvider: Are creds required = [false]\n",
      "2021-02-13 13:51:19,478 INFO  Client: Submitting application application_1613224209876_0002 to ResourceManager\n",
      "2021-02-13 13:51:19,542 INFO  YarnClientImpl: Submitted application application_1613224209876_0002\n",
      "2021-02-13 13:51:19,547 INFO  Client: Application report for application_1613224209876_0002 (state: GENERATING_SECURITY_MATERIAL)\n",
      "2021-02-13 13:51:19,551 INFO  Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1613224279498\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://resourcemanager.service.consul:8088/proxy/application_1613224209876_0002/\n",
      "\t user: blah__meb10179\n",
      "2021-02-13 13:51:19,556 INFO  ShutdownHookManager: Shutdown hook called\n",
      "2021-02-13 13:51:19,557 INFO  ShutdownHookManager: Deleting directory /tmp/spark-4a519839-362d-435d-a164-260306d4d5cb\n",
      "2021-02-13 13:51:19,570 INFO  ShutdownHookManager: Deleting directory /tmp/spark-910cc854-850d-484f-933a-a7065a8eff15\n",
      "\n",
      "stderr: \n",
      "\n",
      "YARN Diagnostics: \n",
      "Invalid host name: local host is: (unknown); destination host is: \"resourcemanager.service.consul\":8032; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost.\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "from datetime import datetime\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import FloatType, StringType\n",
    "import hsfs\n",
    "from hops import hdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tSession 8 unexpectedly reached final status 'dead'. See logs:\n",
      "stdout: \n",
      "2021-02-13 13:51:15,839 WARN  NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2021-02-13 13:51:15,960 WARN  DependencyUtils: Local jar /srv/hops/spark/jars/datanucleus-api.jar does not exist, skipping.\n",
      "2021-02-13 13:51:16,101 INFO  RMProxy: Connecting to ResourceManager at resourcemanager.service.consul/10.0.0.4:8032\n",
      "2021-02-13 13:51:16,890 INFO  Client: Requesting a new application from cluster with 1 NodeManagers\n",
      "2021-02-13 13:51:16,981 INFO  Configuration: found resource resource-types.xml at file:/srv/hops/hadoop-3.2.0.1-EE-RC0/etc/hadoop/resource-types.xml\n",
      "2021-02-13 13:51:16,990 INFO  ResourceUtils: Adding resource type - name = yarn.io/gpu, units = , type = COUNTABLE\n",
      "2021-02-13 13:51:17,007 INFO  Client: Verifying our application has not requested more than the maximum memory capability of the cluster (24271 MB per container)\n",
      "2021-02-13 13:51:17,007 INFO  Client: Will allocate AM container, with 2432 MB memory including 384 MB overhead\n",
      "2021-02-13 13:51:17,008 INFO  Client: Setting up container launch context for our AM\n",
      "2021-02-13 13:51:17,013 INFO  Client: Setting up the launch environment for our AM container\n",
      "2021-02-13 13:51:17,023 INFO  Client: Preparing resources for our AM container\n",
      "2021-02-13 13:51:17,786 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/metrics.properties\n",
      "2021-02-13 13:51:17,863 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/log4j.properties\n",
      "2021-02-13 13:51:17,869 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/hive-site.xml\n",
      "2021-02-13 13:51:18,016 INFO  Client: Uploading resource file:/tmp/spark-4a519839-362d-435d-a164-260306d4d5cb/__spark_conf__5467427819598746872.zip -> hdfs:/Projects/blah/Resources/.sparkStaging/application_1613224209876_0002/__spark_conf__.zip\n",
      "2021-02-13 13:51:18,478 INFO  SecurityManager: Changing view acls to: livy,blah__meb10179\n",
      "2021-02-13 13:51:18,478 INFO  SecurityManager: Changing modify acls to: livy,blah__meb10179\n",
      "2021-02-13 13:51:18,479 INFO  SecurityManager: Changing view acls groups to: \n",
      "2021-02-13 13:51:18,479 INFO  SecurityManager: Changing modify acls groups to: \n",
      "2021-02-13 13:51:18,480 INFO  SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(livy, blah__meb10179); groups with view permissions: Set(); users  with modify permissions: Set(livy, blah__meb10179); groups with modify permissions: Set()\n",
      "2021-02-13 13:51:18,538 INFO  EsServiceCredentialProvider: Loaded EsServiceCredentialProvider\n",
      "2021-02-13 13:51:19,471 INFO  EsServiceCredentialProvider: Hadoop Security Enabled = [false]\n",
      "2021-02-13 13:51:19,471 INFO  EsServiceCredentialProvider: ES Auth Method = [SIMPLE]\n",
      "2021-02-13 13:51:19,471 INFO  EsServiceCredentialProvider: Are creds required = [false]\n",
      "2021-02-13 13:51:19,478 INFO  Client: Submitting application application_1613224209876_0002 to ResourceManager\n",
      "2021-02-13 13:51:19,542 INFO  YarnClientImpl: Submitted application application_1613224209876_0002\n",
      "2021-02-13 13:51:19,547 INFO  Client: Application report for application_1613224209876_0002 (state: GENERATING_SECURITY_MATERIAL)\n",
      "2021-02-13 13:51:19,551 INFO  Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1613224279498\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://resourcemanager.service.consul:8088/proxy/application_1613224209876_0002/\n",
      "\t user: blah__meb10179\n",
      "2021-02-13 13:51:19,556 INFO  ShutdownHookManager: Shutdown hook called\n",
      "2021-02-13 13:51:19,557 INFO  ShutdownHookManager: Deleting directory /tmp/spark-4a519839-362d-435d-a164-260306d4d5cb\n",
      "2021-02-13 13:51:19,570 INFO  ShutdownHookManager: Deleting directory /tmp/spark-910cc854-850d-484f-933a-a7065a8eff15\n",
      "\n",
      "stderr: \n",
      "\n",
      "YARN Diagnostics: \n",
      "Invalid host name: local host is: (unknown); destination host is: \"resourcemanager.service.consul\":8032; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost.\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "def action_2_code(input_str):\n",
    "    x = input_str.split(\"-\")[0]\n",
    "    if (x == \"CASH_IN\"):\n",
    "        node_type = 0\n",
    "    elif (x == \"CASH_OUT\"):\n",
    "        node_type = 1\n",
    "    elif (x == \"DEBIT\"):\n",
    "        node_type = 2\n",
    "    elif (x == \"PAYMENT\"):\n",
    "        node_type = 3\n",
    "    elif (x == \"TRANSFER\"):\n",
    "        node_type = 4\n",
    "    elif (x == \"DEPOSIT\"):\n",
    "        node_type = 4        \n",
    "    else:\n",
    "        node_type = 99\n",
    "    return node_type\n",
    "\n",
    "def party_2_code(x):\n",
    "    if (x == \"Organization\"):\n",
    "        party_type = 0\n",
    "    elif (x == \"Individual\"):\n",
    "        party_type = 1\n",
    "    else:    \n",
    "        party_type = 99\n",
    "    return party_type\n",
    "\n",
    "def timestamp_2_time(x):\n",
    "    dt_obj = datetime.strptime(str(x), '%Y-%m-%d %H:%M:%S')\n",
    "    return dt_obj.strftime(\"%b-%d\") \n",
    "\n",
    "action_2_code_udf = F.udf(action_2_code)\n",
    "party_2_code_udf = F.udf(party_2_code)\n",
    "timestamp_2_time_udf = F.udf(timestamp_2_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a connection to Hopsworks feature store (hsfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tSession 8 unexpectedly reached final status 'dead'. See logs:\n",
      "stdout: \n",
      "2021-02-13 13:51:15,839 WARN  NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2021-02-13 13:51:15,960 WARN  DependencyUtils: Local jar /srv/hops/spark/jars/datanucleus-api.jar does not exist, skipping.\n",
      "2021-02-13 13:51:16,101 INFO  RMProxy: Connecting to ResourceManager at resourcemanager.service.consul/10.0.0.4:8032\n",
      "2021-02-13 13:51:16,890 INFO  Client: Requesting a new application from cluster with 1 NodeManagers\n",
      "2021-02-13 13:51:16,981 INFO  Configuration: found resource resource-types.xml at file:/srv/hops/hadoop-3.2.0.1-EE-RC0/etc/hadoop/resource-types.xml\n",
      "2021-02-13 13:51:16,990 INFO  ResourceUtils: Adding resource type - name = yarn.io/gpu, units = , type = COUNTABLE\n",
      "2021-02-13 13:51:17,007 INFO  Client: Verifying our application has not requested more than the maximum memory capability of the cluster (24271 MB per container)\n",
      "2021-02-13 13:51:17,007 INFO  Client: Will allocate AM container, with 2432 MB memory including 384 MB overhead\n",
      "2021-02-13 13:51:17,008 INFO  Client: Setting up container launch context for our AM\n",
      "2021-02-13 13:51:17,013 INFO  Client: Setting up the launch environment for our AM container\n",
      "2021-02-13 13:51:17,023 INFO  Client: Preparing resources for our AM container\n",
      "2021-02-13 13:51:17,786 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/metrics.properties\n",
      "2021-02-13 13:51:17,863 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/log4j.properties\n",
      "2021-02-13 13:51:17,869 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/hive-site.xml\n",
      "2021-02-13 13:51:18,016 INFO  Client: Uploading resource file:/tmp/spark-4a519839-362d-435d-a164-260306d4d5cb/__spark_conf__5467427819598746872.zip -> hdfs:/Projects/blah/Resources/.sparkStaging/application_1613224209876_0002/__spark_conf__.zip\n",
      "2021-02-13 13:51:18,478 INFO  SecurityManager: Changing view acls to: livy,blah__meb10179\n",
      "2021-02-13 13:51:18,478 INFO  SecurityManager: Changing modify acls to: livy,blah__meb10179\n",
      "2021-02-13 13:51:18,479 INFO  SecurityManager: Changing view acls groups to: \n",
      "2021-02-13 13:51:18,479 INFO  SecurityManager: Changing modify acls groups to: \n",
      "2021-02-13 13:51:18,480 INFO  SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(livy, blah__meb10179); groups with view permissions: Set(); users  with modify permissions: Set(livy, blah__meb10179); groups with modify permissions: Set()\n",
      "2021-02-13 13:51:18,538 INFO  EsServiceCredentialProvider: Loaded EsServiceCredentialProvider\n",
      "2021-02-13 13:51:19,471 INFO  EsServiceCredentialProvider: Hadoop Security Enabled = [false]\n",
      "2021-02-13 13:51:19,471 INFO  EsServiceCredentialProvider: ES Auth Method = [SIMPLE]\n",
      "2021-02-13 13:51:19,471 INFO  EsServiceCredentialProvider: Are creds required = [false]\n",
      "2021-02-13 13:51:19,478 INFO  Client: Submitting application application_1613224209876_0002 to ResourceManager\n",
      "2021-02-13 13:51:19,542 INFO  YarnClientImpl: Submitted application application_1613224209876_0002\n",
      "2021-02-13 13:51:19,547 INFO  Client: Application report for application_1613224209876_0002 (state: GENERATING_SECURITY_MATERIAL)\n",
      "2021-02-13 13:51:19,551 INFO  Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1613224279498\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://resourcemanager.service.consul:8088/proxy/application_1613224209876_0002/\n",
      "\t user: blah__meb10179\n",
      "2021-02-13 13:51:19,556 INFO  ShutdownHookManager: Shutdown hook called\n",
      "2021-02-13 13:51:19,557 INFO  ShutdownHookManager: Deleting directory /tmp/spark-4a519839-362d-435d-a164-260306d4d5cb\n",
      "2021-02-13 13:51:19,570 INFO  ShutdownHookManager: Deleting directory /tmp/spark-910cc854-850d-484f-933a-a7065a8eff15\n",
      "\n",
      "stderr: \n",
      "\n",
      "YARN Diagnostics: \n",
      "Invalid host name: local host is: (unknown); destination host is: \"resourcemanager.service.consul\":8032; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost.\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "# Create a connection\n",
    "connection = hsfs.connection()\n",
    "# Get the feature store handle for the project's feature store\n",
    "fs = connection.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load transactions datasets as spark dataframe and perform feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tSession 8 unexpectedly reached final status 'dead'. See logs:\n",
      "stdout: \n",
      "2021-02-13 13:51:15,839 WARN  NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2021-02-13 13:51:15,960 WARN  DependencyUtils: Local jar /srv/hops/spark/jars/datanucleus-api.jar does not exist, skipping.\n",
      "2021-02-13 13:51:16,101 INFO  RMProxy: Connecting to ResourceManager at resourcemanager.service.consul/10.0.0.4:8032\n",
      "2021-02-13 13:51:16,890 INFO  Client: Requesting a new application from cluster with 1 NodeManagers\n",
      "2021-02-13 13:51:16,981 INFO  Configuration: found resource resource-types.xml at file:/srv/hops/hadoop-3.2.0.1-EE-RC0/etc/hadoop/resource-types.xml\n",
      "2021-02-13 13:51:16,990 INFO  ResourceUtils: Adding resource type - name = yarn.io/gpu, units = , type = COUNTABLE\n",
      "2021-02-13 13:51:17,007 INFO  Client: Verifying our application has not requested more than the maximum memory capability of the cluster (24271 MB per container)\n",
      "2021-02-13 13:51:17,007 INFO  Client: Will allocate AM container, with 2432 MB memory including 384 MB overhead\n",
      "2021-02-13 13:51:17,008 INFO  Client: Setting up container launch context for our AM\n",
      "2021-02-13 13:51:17,013 INFO  Client: Setting up the launch environment for our AM container\n",
      "2021-02-13 13:51:17,023 INFO  Client: Preparing resources for our AM container\n",
      "2021-02-13 13:51:17,786 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/metrics.properties\n",
      "2021-02-13 13:51:17,863 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/log4j.properties\n",
      "2021-02-13 13:51:17,869 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/hive-site.xml\n",
      "2021-02-13 13:51:18,016 INFO  Client: Uploading resource file:/tmp/spark-4a519839-362d-435d-a164-260306d4d5cb/__spark_conf__5467427819598746872.zip -> hdfs:/Projects/blah/Resources/.sparkStaging/application_1613224209876_0002/__spark_conf__.zip\n",
      "2021-02-13 13:51:18,478 INFO  SecurityManager: Changing view acls to: livy,blah__meb10179\n",
      "2021-02-13 13:51:18,478 INFO  SecurityManager: Changing modify acls to: livy,blah__meb10179\n",
      "2021-02-13 13:51:18,479 INFO  SecurityManager: Changing view acls groups to: \n",
      "2021-02-13 13:51:18,479 INFO  SecurityManager: Changing modify acls groups to: \n",
      "2021-02-13 13:51:18,480 INFO  SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(livy, blah__meb10179); groups with view permissions: Set(); users  with modify permissions: Set(livy, blah__meb10179); groups with modify permissions: Set()\n",
      "2021-02-13 13:51:18,538 INFO  EsServiceCredentialProvider: Loaded EsServiceCredentialProvider\n",
      "2021-02-13 13:51:19,471 INFO  EsServiceCredentialProvider: Hadoop Security Enabled = [false]\n",
      "2021-02-13 13:51:19,471 INFO  EsServiceCredentialProvider: ES Auth Method = [SIMPLE]\n",
      "2021-02-13 13:51:19,471 INFO  EsServiceCredentialProvider: Are creds required = [false]\n",
      "2021-02-13 13:51:19,478 INFO  Client: Submitting application application_1613224209876_0002 to ResourceManager\n",
      "2021-02-13 13:51:19,542 INFO  YarnClientImpl: Submitted application application_1613224209876_0002\n",
      "2021-02-13 13:51:19,547 INFO  Client: Application report for application_1613224209876_0002 (state: GENERATING_SECURITY_MATERIAL)\n",
      "2021-02-13 13:51:19,551 INFO  Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1613224279498\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://resourcemanager.service.consul:8088/proxy/application_1613224209876_0002/\n",
      "\t user: blah__meb10179\n",
      "2021-02-13 13:51:19,556 INFO  ShutdownHookManager: Shutdown hook called\n",
      "2021-02-13 13:51:19,557 INFO  ShutdownHookManager: Deleting directory /tmp/spark-4a519839-362d-435d-a164-260306d4d5cb\n",
      "2021-02-13 13:51:19,570 INFO  ShutdownHookManager: Deleting directory /tmp/spark-910cc854-850d-484f-933a-a7065a8eff15\n",
      "\n",
      "stderr: \n",
      "\n",
      "YARN Diagnostics: \n",
      "Invalid host name: local host is: (unknown); destination host is: \"resourcemanager.service.consul\":8032; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost.\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "transactions_df = spark.read\\\n",
    "             .option(\"inferSchema\", \"true\")\\\n",
    "             .option(\"header\", \"true\")\\\n",
    "             .format(\"csv\")\\\n",
    "             .load(\"hdfs:///Projects/{}/Resources/transactions.csv\".format(hdfs.project_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tSession 8 unexpectedly reached final status 'dead'. See logs:\n",
      "stdout: \n",
      "2021-02-13 13:51:15,839 WARN  NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2021-02-13 13:51:15,960 WARN  DependencyUtils: Local jar /srv/hops/spark/jars/datanucleus-api.jar does not exist, skipping.\n",
      "2021-02-13 13:51:16,101 INFO  RMProxy: Connecting to ResourceManager at resourcemanager.service.consul/10.0.0.4:8032\n",
      "2021-02-13 13:51:16,890 INFO  Client: Requesting a new application from cluster with 1 NodeManagers\n",
      "2021-02-13 13:51:16,981 INFO  Configuration: found resource resource-types.xml at file:/srv/hops/hadoop-3.2.0.1-EE-RC0/etc/hadoop/resource-types.xml\n",
      "2021-02-13 13:51:16,990 INFO  ResourceUtils: Adding resource type - name = yarn.io/gpu, units = , type = COUNTABLE\n",
      "2021-02-13 13:51:17,007 INFO  Client: Verifying our application has not requested more than the maximum memory capability of the cluster (24271 MB per container)\n",
      "2021-02-13 13:51:17,007 INFO  Client: Will allocate AM container, with 2432 MB memory including 384 MB overhead\n",
      "2021-02-13 13:51:17,008 INFO  Client: Setting up container launch context for our AM\n",
      "2021-02-13 13:51:17,013 INFO  Client: Setting up the launch environment for our AM container\n",
      "2021-02-13 13:51:17,023 INFO  Client: Preparing resources for our AM container\n",
      "2021-02-13 13:51:17,786 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/metrics.properties\n",
      "2021-02-13 13:51:17,863 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/log4j.properties\n",
      "2021-02-13 13:51:17,869 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/hive-site.xml\n",
      "2021-02-13 13:51:18,016 INFO  Client: Uploading resource file:/tmp/spark-4a519839-362d-435d-a164-260306d4d5cb/__spark_conf__5467427819598746872.zip -> hdfs:/Projects/blah/Resources/.sparkStaging/application_1613224209876_0002/__spark_conf__.zip\n",
      "2021-02-13 13:51:18,478 INFO  SecurityManager: Changing view acls to: livy,blah__meb10179\n",
      "2021-02-13 13:51:18,478 INFO  SecurityManager: Changing modify acls to: livy,blah__meb10179\n",
      "2021-02-13 13:51:18,479 INFO  SecurityManager: Changing view acls groups to: \n",
      "2021-02-13 13:51:18,479 INFO  SecurityManager: Changing modify acls groups to: \n",
      "2021-02-13 13:51:18,480 INFO  SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(livy, blah__meb10179); groups with view permissions: Set(); users  with modify permissions: Set(livy, blah__meb10179); groups with modify permissions: Set()\n",
      "2021-02-13 13:51:18,538 INFO  EsServiceCredentialProvider: Loaded EsServiceCredentialProvider\n",
      "2021-02-13 13:51:19,471 INFO  EsServiceCredentialProvider: Hadoop Security Enabled = [false]\n",
      "2021-02-13 13:51:19,471 INFO  EsServiceCredentialProvider: ES Auth Method = [SIMPLE]\n",
      "2021-02-13 13:51:19,471 INFO  EsServiceCredentialProvider: Are creds required = [false]\n",
      "2021-02-13 13:51:19,478 INFO  Client: Submitting application application_1613224209876_0002 to ResourceManager\n",
      "2021-02-13 13:51:19,542 INFO  YarnClientImpl: Submitted application application_1613224209876_0002\n",
      "2021-02-13 13:51:19,547 INFO  Client: Application report for application_1613224209876_0002 (state: GENERATING_SECURITY_MATERIAL)\n",
      "2021-02-13 13:51:19,551 INFO  Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1613224279498\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://resourcemanager.service.consul:8088/proxy/application_1613224209876_0002/\n",
      "\t user: blah__meb10179\n",
      "2021-02-13 13:51:19,556 INFO  ShutdownHookManager: Shutdown hook called\n",
      "2021-02-13 13:51:19,557 INFO  ShutdownHookManager: Deleting directory /tmp/spark-4a519839-362d-435d-a164-260306d4d5cb\n",
      "2021-02-13 13:51:19,570 INFO  ShutdownHookManager: Deleting directory /tmp/spark-910cc854-850d-484f-933a-a7065a8eff15\n",
      "\n",
      "stderr: \n",
      "\n",
      "YARN Diagnostics: \n",
      "Invalid host name: local host is: (unknown); destination host is: \"resourcemanager.service.consul\":8032; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost.\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "transactions_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tSession 8 unexpectedly reached final status 'dead'. See logs:\n",
      "stdout: \n",
      "2021-02-13 13:51:15,839 WARN  NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2021-02-13 13:51:15,960 WARN  DependencyUtils: Local jar /srv/hops/spark/jars/datanucleus-api.jar does not exist, skipping.\n",
      "2021-02-13 13:51:16,101 INFO  RMProxy: Connecting to ResourceManager at resourcemanager.service.consul/10.0.0.4:8032\n",
      "2021-02-13 13:51:16,890 INFO  Client: Requesting a new application from cluster with 1 NodeManagers\n",
      "2021-02-13 13:51:16,981 INFO  Configuration: found resource resource-types.xml at file:/srv/hops/hadoop-3.2.0.1-EE-RC0/etc/hadoop/resource-types.xml\n",
      "2021-02-13 13:51:16,990 INFO  ResourceUtils: Adding resource type - name = yarn.io/gpu, units = , type = COUNTABLE\n",
      "2021-02-13 13:51:17,007 INFO  Client: Verifying our application has not requested more than the maximum memory capability of the cluster (24271 MB per container)\n",
      "2021-02-13 13:51:17,007 INFO  Client: Will allocate AM container, with 2432 MB memory including 384 MB overhead\n",
      "2021-02-13 13:51:17,008 INFO  Client: Setting up container launch context for our AM\n",
      "2021-02-13 13:51:17,013 INFO  Client: Setting up the launch environment for our AM container\n",
      "2021-02-13 13:51:17,023 INFO  Client: Preparing resources for our AM container\n",
      "2021-02-13 13:51:17,786 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/metrics.properties\n",
      "2021-02-13 13:51:17,863 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/log4j.properties\n",
      "2021-02-13 13:51:17,869 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/hive-site.xml\n",
      "2021-02-13 13:51:18,016 INFO  Client: Uploading resource file:/tmp/spark-4a519839-362d-435d-a164-260306d4d5cb/__spark_conf__5467427819598746872.zip -> hdfs:/Projects/blah/Resources/.sparkStaging/application_1613224209876_0002/__spark_conf__.zip\n",
      "2021-02-13 13:51:18,478 INFO  SecurityManager: Changing view acls to: livy,blah__meb10179\n",
      "2021-02-13 13:51:18,478 INFO  SecurityManager: Changing modify acls to: livy,blah__meb10179\n",
      "2021-02-13 13:51:18,479 INFO  SecurityManager: Changing view acls groups to: \n",
      "2021-02-13 13:51:18,479 INFO  SecurityManager: Changing modify acls groups to: \n",
      "2021-02-13 13:51:18,480 INFO  SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(livy, blah__meb10179); groups with view permissions: Set(); users  with modify permissions: Set(livy, blah__meb10179); groups with modify permissions: Set()\n",
      "2021-02-13 13:51:18,538 INFO  EsServiceCredentialProvider: Loaded EsServiceCredentialProvider\n",
      "2021-02-13 13:51:19,471 INFO  EsServiceCredentialProvider: Hadoop Security Enabled = [false]\n",
      "2021-02-13 13:51:19,471 INFO  EsServiceCredentialProvider: ES Auth Method = [SIMPLE]\n",
      "2021-02-13 13:51:19,471 INFO  EsServiceCredentialProvider: Are creds required = [false]\n",
      "2021-02-13 13:51:19,478 INFO  Client: Submitting application application_1613224209876_0002 to ResourceManager\n",
      "2021-02-13 13:51:19,542 INFO  YarnClientImpl: Submitted application application_1613224209876_0002\n",
      "2021-02-13 13:51:19,547 INFO  Client: Application report for application_1613224209876_0002 (state: GENERATING_SECURITY_MATERIAL)\n",
      "2021-02-13 13:51:19,551 INFO  Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1613224279498\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://resourcemanager.service.consul:8088/proxy/application_1613224209876_0002/\n",
      "\t user: blah__meb10179\n",
      "2021-02-13 13:51:19,556 INFO  ShutdownHookManager: Shutdown hook called\n",
      "2021-02-13 13:51:19,557 INFO  ShutdownHookManager: Deleting directory /tmp/spark-4a519839-362d-435d-a164-260306d4d5cb\n",
      "2021-02-13 13:51:19,570 INFO  ShutdownHookManager: Deleting directory /tmp/spark-910cc854-850d-484f-933a-a7065a8eff15\n",
      "\n",
      "stderr: \n",
      "\n",
      "YARN Diagnostics: \n",
      "Invalid host name: local host is: (unknown); destination host is: \"resourcemanager.service.consul\":8032; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost.\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "transactions_df = transactions_df.withColumn('tx_type', action_2_code_udf(F.col('tx_type')))\\\n",
    "                                 .withColumn('tran_timestamp', timestamp_2_time_udf(F.col('tran_timestamp')).cast(StringType()))\\\n",
    "                                 .withColumnRenamed(\"orig_acct\",\"source\")\\\n",
    "                                 .withColumnRenamed(\"bene_acct\",\"target\")\\\n",
    "                                 .select(\"src\",\"dst\",\"tran_timestamp\",\"tran_id\",\"tx_type\",\"base_amt\")\n",
    "transactions_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create transactions feature group metadata and save it in to hsfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tSession 8 unexpectedly reached final status 'dead'. See logs:\n",
      "stdout: \n",
      "2021-02-13 13:51:15,839 WARN  NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2021-02-13 13:51:15,960 WARN  DependencyUtils: Local jar /srv/hops/spark/jars/datanucleus-api.jar does not exist, skipping.\n",
      "2021-02-13 13:51:16,101 INFO  RMProxy: Connecting to ResourceManager at resourcemanager.service.consul/10.0.0.4:8032\n",
      "2021-02-13 13:51:16,890 INFO  Client: Requesting a new application from cluster with 1 NodeManagers\n",
      "2021-02-13 13:51:16,981 INFO  Configuration: found resource resource-types.xml at file:/srv/hops/hadoop-3.2.0.1-EE-RC0/etc/hadoop/resource-types.xml\n",
      "2021-02-13 13:51:16,990 INFO  ResourceUtils: Adding resource type - name = yarn.io/gpu, units = , type = COUNTABLE\n",
      "2021-02-13 13:51:17,007 INFO  Client: Verifying our application has not requested more than the maximum memory capability of the cluster (24271 MB per container)\n",
      "2021-02-13 13:51:17,007 INFO  Client: Will allocate AM container, with 2432 MB memory including 384 MB overhead\n",
      "2021-02-13 13:51:17,008 INFO  Client: Setting up container launch context for our AM\n",
      "2021-02-13 13:51:17,013 INFO  Client: Setting up the launch environment for our AM container\n",
      "2021-02-13 13:51:17,023 INFO  Client: Preparing resources for our AM container\n",
      "2021-02-13 13:51:17,786 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/metrics.properties\n",
      "2021-02-13 13:51:17,863 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/log4j.properties\n",
      "2021-02-13 13:51:17,869 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/hive-site.xml\n",
      "2021-02-13 13:51:18,016 INFO  Client: Uploading resource file:/tmp/spark-4a519839-362d-435d-a164-260306d4d5cb/__spark_conf__5467427819598746872.zip -> hdfs:/Projects/blah/Resources/.sparkStaging/application_1613224209876_0002/__spark_conf__.zip\n",
      "2021-02-13 13:51:18,478 INFO  SecurityManager: Changing view acls to: livy,blah__meb10179\n",
      "2021-02-13 13:51:18,478 INFO  SecurityManager: Changing modify acls to: livy,blah__meb10179\n",
      "2021-02-13 13:51:18,479 INFO  SecurityManager: Changing view acls groups to: \n",
      "2021-02-13 13:51:18,479 INFO  SecurityManager: Changing modify acls groups to: \n",
      "2021-02-13 13:51:18,480 INFO  SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(livy, blah__meb10179); groups with view permissions: Set(); users  with modify permissions: Set(livy, blah__meb10179); groups with modify permissions: Set()\n",
      "2021-02-13 13:51:18,538 INFO  EsServiceCredentialProvider: Loaded EsServiceCredentialProvider\n",
      "2021-02-13 13:51:19,471 INFO  EsServiceCredentialProvider: Hadoop Security Enabled = [false]\n",
      "2021-02-13 13:51:19,471 INFO  EsServiceCredentialProvider: ES Auth Method = [SIMPLE]\n",
      "2021-02-13 13:51:19,471 INFO  EsServiceCredentialProvider: Are creds required = [false]\n",
      "2021-02-13 13:51:19,478 INFO  Client: Submitting application application_1613224209876_0002 to ResourceManager\n",
      "2021-02-13 13:51:19,542 INFO  YarnClientImpl: Submitted application application_1613224209876_0002\n",
      "2021-02-13 13:51:19,547 INFO  Client: Application report for application_1613224209876_0002 (state: GENERATING_SECURITY_MATERIAL)\n",
      "2021-02-13 13:51:19,551 INFO  Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1613224279498\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://resourcemanager.service.consul:8088/proxy/application_1613224209876_0002/\n",
      "\t user: blah__meb10179\n",
      "2021-02-13 13:51:19,556 INFO  ShutdownHookManager: Shutdown hook called\n",
      "2021-02-13 13:51:19,557 INFO  ShutdownHookManager: Deleting directory /tmp/spark-4a519839-362d-435d-a164-260306d4d5cb\n",
      "2021-02-13 13:51:19,570 INFO  ShutdownHookManager: Deleting directory /tmp/spark-910cc854-850d-484f-933a-a7065a8eff15\n",
      "\n",
      "stderr: \n",
      "\n",
      "YARN Diagnostics: \n",
      "Invalid host name: local host is: (unknown); destination host is: \"resourcemanager.service.consul\":8032; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost.\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "transactions_fg = fs.create_feature_group(name=\"transactions_fg\",\n",
    "                                       version=1,\n",
    "                                       primary_key=[\"tran_id\"],\n",
    "#                                       partition_key=[\"tran_timestamp\"],   \n",
    "                                       description=\"transactions features\",\n",
    "                                       time_travel_format=None,                                        \n",
    "                                       statistics_config=False)\n",
    "transactions_fg.save(transactions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load alert transactions datasets as spark dataframe and perform feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tSession 8 unexpectedly reached final status 'dead'. See logs:\n",
      "stdout: \n",
      "2021-02-13 13:51:15,839 WARN  NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2021-02-13 13:51:15,960 WARN  DependencyUtils: Local jar /srv/hops/spark/jars/datanucleus-api.jar does not exist, skipping.\n",
      "2021-02-13 13:51:16,101 INFO  RMProxy: Connecting to ResourceManager at resourcemanager.service.consul/10.0.0.4:8032\n",
      "2021-02-13 13:51:16,890 INFO  Client: Requesting a new application from cluster with 1 NodeManagers\n",
      "2021-02-13 13:51:16,981 INFO  Configuration: found resource resource-types.xml at file:/srv/hops/hadoop-3.2.0.1-EE-RC0/etc/hadoop/resource-types.xml\n",
      "2021-02-13 13:51:16,990 INFO  ResourceUtils: Adding resource type - name = yarn.io/gpu, units = , type = COUNTABLE\n",
      "2021-02-13 13:51:17,007 INFO  Client: Verifying our application has not requested more than the maximum memory capability of the cluster (24271 MB per container)\n",
      "2021-02-13 13:51:17,007 INFO  Client: Will allocate AM container, with 2432 MB memory including 384 MB overhead\n",
      "2021-02-13 13:51:17,008 INFO  Client: Setting up container launch context for our AM\n",
      "2021-02-13 13:51:17,013 INFO  Client: Setting up the launch environment for our AM container\n",
      "2021-02-13 13:51:17,023 INFO  Client: Preparing resources for our AM container\n",
      "2021-02-13 13:51:17,786 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/metrics.properties\n",
      "2021-02-13 13:51:17,863 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/log4j.properties\n",
      "2021-02-13 13:51:17,869 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/hive-site.xml\n",
      "2021-02-13 13:51:18,016 INFO  Client: Uploading resource file:/tmp/spark-4a519839-362d-435d-a164-260306d4d5cb/__spark_conf__5467427819598746872.zip -> hdfs:/Projects/blah/Resources/.sparkStaging/application_1613224209876_0002/__spark_conf__.zip\n",
      "2021-02-13 13:51:18,478 INFO  SecurityManager: Changing view acls to: livy,blah__meb10179\n",
      "2021-02-13 13:51:18,478 INFO  SecurityManager: Changing modify acls to: livy,blah__meb10179\n",
      "2021-02-13 13:51:18,479 INFO  SecurityManager: Changing view acls groups to: \n",
      "2021-02-13 13:51:18,479 INFO  SecurityManager: Changing modify acls groups to: \n",
      "2021-02-13 13:51:18,480 INFO  SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(livy, blah__meb10179); groups with view permissions: Set(); users  with modify permissions: Set(livy, blah__meb10179); groups with modify permissions: Set()\n",
      "2021-02-13 13:51:18,538 INFO  EsServiceCredentialProvider: Loaded EsServiceCredentialProvider\n",
      "2021-02-13 13:51:19,471 INFO  EsServiceCredentialProvider: Hadoop Security Enabled = [false]\n",
      "2021-02-13 13:51:19,471 INFO  EsServiceCredentialProvider: ES Auth Method = [SIMPLE]\n",
      "2021-02-13 13:51:19,471 INFO  EsServiceCredentialProvider: Are creds required = [false]\n",
      "2021-02-13 13:51:19,478 INFO  Client: Submitting application application_1613224209876_0002 to ResourceManager\n",
      "2021-02-13 13:51:19,542 INFO  YarnClientImpl: Submitted application application_1613224209876_0002\n",
      "2021-02-13 13:51:19,547 INFO  Client: Application report for application_1613224209876_0002 (state: GENERATING_SECURITY_MATERIAL)\n",
      "2021-02-13 13:51:19,551 INFO  Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1613224279498\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://resourcemanager.service.consul:8088/proxy/application_1613224209876_0002/\n",
      "\t user: blah__meb10179\n",
      "2021-02-13 13:51:19,556 INFO  ShutdownHookManager: Shutdown hook called\n",
      "2021-02-13 13:51:19,557 INFO  ShutdownHookManager: Deleting directory /tmp/spark-4a519839-362d-435d-a164-260306d4d5cb\n",
      "2021-02-13 13:51:19,570 INFO  ShutdownHookManager: Deleting directory /tmp/spark-910cc854-850d-484f-933a-a7065a8eff15\n",
      "\n",
      "stderr: \n",
      "\n",
      "YARN Diagnostics: \n",
      "Invalid host name: local host is: (unknown); destination host is: \"resourcemanager.service.consul\":8032; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost.\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "alert_transactions = spark.read\\\n",
    "             .option(\"inferSchema\", \"true\")\\\n",
    "             .option(\"header\", \"true\")\\\n",
    "             .format(\"csv\")\\\n",
    "             .load(\"hdfs:///Projects/{}/Resources/alert_transactions.csv\".format(hdfs.project_name()))\n",
    "alert_transactions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tSession 8 unexpectedly reached final status 'dead'. See logs:\n",
      "stdout: \n",
      "2021-02-13 13:51:15,839 WARN  NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2021-02-13 13:51:15,960 WARN  DependencyUtils: Local jar /srv/hops/spark/jars/datanucleus-api.jar does not exist, skipping.\n",
      "2021-02-13 13:51:16,101 INFO  RMProxy: Connecting to ResourceManager at resourcemanager.service.consul/10.0.0.4:8032\n",
      "2021-02-13 13:51:16,890 INFO  Client: Requesting a new application from cluster with 1 NodeManagers\n",
      "2021-02-13 13:51:16,981 INFO  Configuration: found resource resource-types.xml at file:/srv/hops/hadoop-3.2.0.1-EE-RC0/etc/hadoop/resource-types.xml\n",
      "2021-02-13 13:51:16,990 INFO  ResourceUtils: Adding resource type - name = yarn.io/gpu, units = , type = COUNTABLE\n",
      "2021-02-13 13:51:17,007 INFO  Client: Verifying our application has not requested more than the maximum memory capability of the cluster (24271 MB per container)\n",
      "2021-02-13 13:51:17,007 INFO  Client: Will allocate AM container, with 2432 MB memory including 384 MB overhead\n",
      "2021-02-13 13:51:17,008 INFO  Client: Setting up container launch context for our AM\n",
      "2021-02-13 13:51:17,013 INFO  Client: Setting up the launch environment for our AM container\n",
      "2021-02-13 13:51:17,023 INFO  Client: Preparing resources for our AM container\n",
      "2021-02-13 13:51:17,786 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/metrics.properties\n",
      "2021-02-13 13:51:17,863 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/log4j.properties\n",
      "2021-02-13 13:51:17,869 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/hive-site.xml\n",
      "2021-02-13 13:51:18,016 INFO  Client: Uploading resource file:/tmp/spark-4a519839-362d-435d-a164-260306d4d5cb/__spark_conf__5467427819598746872.zip -> hdfs:/Projects/blah/Resources/.sparkStaging/application_1613224209876_0002/__spark_conf__.zip\n",
      "2021-02-13 13:51:18,478 INFO  SecurityManager: Changing view acls to: livy,blah__meb10179\n",
      "2021-02-13 13:51:18,478 INFO  SecurityManager: Changing modify acls to: livy,blah__meb10179\n",
      "2021-02-13 13:51:18,479 INFO  SecurityManager: Changing view acls groups to: \n",
      "2021-02-13 13:51:18,479 INFO  SecurityManager: Changing modify acls groups to: \n",
      "2021-02-13 13:51:18,480 INFO  SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(livy, blah__meb10179); groups with view permissions: Set(); users  with modify permissions: Set(livy, blah__meb10179); groups with modify permissions: Set()\n",
      "2021-02-13 13:51:18,538 INFO  EsServiceCredentialProvider: Loaded EsServiceCredentialProvider\n",
      "2021-02-13 13:51:19,471 INFO  EsServiceCredentialProvider: Hadoop Security Enabled = [false]\n",
      "2021-02-13 13:51:19,471 INFO  EsServiceCredentialProvider: ES Auth Method = [SIMPLE]\n",
      "2021-02-13 13:51:19,471 INFO  EsServiceCredentialProvider: Are creds required = [false]\n",
      "2021-02-13 13:51:19,478 INFO  Client: Submitting application application_1613224209876_0002 to ResourceManager\n",
      "2021-02-13 13:51:19,542 INFO  YarnClientImpl: Submitted application application_1613224209876_0002\n",
      "2021-02-13 13:51:19,547 INFO  Client: Application report for application_1613224209876_0002 (state: GENERATING_SECURITY_MATERIAL)\n",
      "2021-02-13 13:51:19,551 INFO  Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1613224279498\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://resourcemanager.service.consul:8088/proxy/application_1613224209876_0002/\n",
      "\t user: blah__meb10179\n",
      "2021-02-13 13:51:19,556 INFO  ShutdownHookManager: Shutdown hook called\n",
      "2021-02-13 13:51:19,557 INFO  ShutdownHookManager: Deleting directory /tmp/spark-4a519839-362d-435d-a164-260306d4d5cb\n",
      "2021-02-13 13:51:19,570 INFO  ShutdownHookManager: Deleting directory /tmp/spark-910cc854-850d-484f-933a-a7065a8eff15\n",
      "\n",
      "stderr: \n",
      "\n",
      "YARN Diagnostics: \n",
      "Invalid host name: local host is: (unknown); destination host is: \"resourcemanager.service.consul\":8032; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost.\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "alert_transactions = alert_transactions.select(\"alert_id\",\"alert_type\",\"is_sar\",\"tran_id\").orderBy(\"tran_id\")\n",
    "alert_transactions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create alert transactions feature group metadata and perform feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tSession 8 unexpectedly reached final status 'dead'. See logs:\n",
      "stdout: \n",
      "2021-02-13 13:51:15,839 WARN  NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2021-02-13 13:51:15,960 WARN  DependencyUtils: Local jar /srv/hops/spark/jars/datanucleus-api.jar does not exist, skipping.\n",
      "2021-02-13 13:51:16,101 INFO  RMProxy: Connecting to ResourceManager at resourcemanager.service.consul/10.0.0.4:8032\n",
      "2021-02-13 13:51:16,890 INFO  Client: Requesting a new application from cluster with 1 NodeManagers\n",
      "2021-02-13 13:51:16,981 INFO  Configuration: found resource resource-types.xml at file:/srv/hops/hadoop-3.2.0.1-EE-RC0/etc/hadoop/resource-types.xml\n",
      "2021-02-13 13:51:16,990 INFO  ResourceUtils: Adding resource type - name = yarn.io/gpu, units = , type = COUNTABLE\n",
      "2021-02-13 13:51:17,007 INFO  Client: Verifying our application has not requested more than the maximum memory capability of the cluster (24271 MB per container)\n",
      "2021-02-13 13:51:17,007 INFO  Client: Will allocate AM container, with 2432 MB memory including 384 MB overhead\n",
      "2021-02-13 13:51:17,008 INFO  Client: Setting up container launch context for our AM\n",
      "2021-02-13 13:51:17,013 INFO  Client: Setting up the launch environment for our AM container\n",
      "2021-02-13 13:51:17,023 INFO  Client: Preparing resources for our AM container\n",
      "2021-02-13 13:51:17,786 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/metrics.properties\n",
      "2021-02-13 13:51:17,863 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/log4j.properties\n",
      "2021-02-13 13:51:17,869 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/hive-site.xml\n",
      "2021-02-13 13:51:18,016 INFO  Client: Uploading resource file:/tmp/spark-4a519839-362d-435d-a164-260306d4d5cb/__spark_conf__5467427819598746872.zip -> hdfs:/Projects/blah/Resources/.sparkStaging/application_1613224209876_0002/__spark_conf__.zip\n",
      "2021-02-13 13:51:18,478 INFO  SecurityManager: Changing view acls to: livy,blah__meb10179\n",
      "2021-02-13 13:51:18,478 INFO  SecurityManager: Changing modify acls to: livy,blah__meb10179\n",
      "2021-02-13 13:51:18,479 INFO  SecurityManager: Changing view acls groups to: \n",
      "2021-02-13 13:51:18,479 INFO  SecurityManager: Changing modify acls groups to: \n",
      "2021-02-13 13:51:18,480 INFO  SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(livy, blah__meb10179); groups with view permissions: Set(); users  with modify permissions: Set(livy, blah__meb10179); groups with modify permissions: Set()\n",
      "2021-02-13 13:51:18,538 INFO  EsServiceCredentialProvider: Loaded EsServiceCredentialProvider\n",
      "2021-02-13 13:51:19,471 INFO  EsServiceCredentialProvider: Hadoop Security Enabled = [false]\n",
      "2021-02-13 13:51:19,471 INFO  EsServiceCredentialProvider: ES Auth Method = [SIMPLE]\n",
      "2021-02-13 13:51:19,471 INFO  EsServiceCredentialProvider: Are creds required = [false]\n",
      "2021-02-13 13:51:19,478 INFO  Client: Submitting application application_1613224209876_0002 to ResourceManager\n",
      "2021-02-13 13:51:19,542 INFO  YarnClientImpl: Submitted application application_1613224209876_0002\n",
      "2021-02-13 13:51:19,547 INFO  Client: Application report for application_1613224209876_0002 (state: GENERATING_SECURITY_MATERIAL)\n",
      "2021-02-13 13:51:19,551 INFO  Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1613224279498\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://resourcemanager.service.consul:8088/proxy/application_1613224209876_0002/\n",
      "\t user: blah__meb10179\n",
      "2021-02-13 13:51:19,556 INFO  ShutdownHookManager: Shutdown hook called\n",
      "2021-02-13 13:51:19,557 INFO  ShutdownHookManager: Deleting directory /tmp/spark-4a519839-362d-435d-a164-260306d4d5cb\n",
      "2021-02-13 13:51:19,570 INFO  ShutdownHookManager: Deleting directory /tmp/spark-910cc854-850d-484f-933a-a7065a8eff15\n",
      "\n",
      "stderr: \n",
      "\n",
      "YARN Diagnostics: \n",
      "Invalid host name: local host is: (unknown); destination host is: \"resourcemanager.service.consul\":8032; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost.\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "alert_transactions_fg = fs.create_feature_group(name=\"alert_transactions_fg\",\n",
    "                                       version=1,\n",
    "                                       primary_key=[\"tran_id\"],\n",
    "#                                       partition_key=[\"alert_type\"],         \n",
    "                                       description=\"alert transactions\",\n",
    "                                       time_travel_format=None,                                        \n",
    "                                       statistics_config=False)\n",
    "alert_transactions_fg.save(alert_transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load party datasets as spark dataframe and ingest into hsfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tSession 8 unexpectedly reached final status 'dead'. See logs:\n",
      "stdout: \n",
      "2021-02-13 13:51:15,839 WARN  NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2021-02-13 13:51:15,960 WARN  DependencyUtils: Local jar /srv/hops/spark/jars/datanucleus-api.jar does not exist, skipping.\n",
      "2021-02-13 13:51:16,101 INFO  RMProxy: Connecting to ResourceManager at resourcemanager.service.consul/10.0.0.4:8032\n",
      "2021-02-13 13:51:16,890 INFO  Client: Requesting a new application from cluster with 1 NodeManagers\n",
      "2021-02-13 13:51:16,981 INFO  Configuration: found resource resource-types.xml at file:/srv/hops/hadoop-3.2.0.1-EE-RC0/etc/hadoop/resource-types.xml\n",
      "2021-02-13 13:51:16,990 INFO  ResourceUtils: Adding resource type - name = yarn.io/gpu, units = , type = COUNTABLE\n",
      "2021-02-13 13:51:17,007 INFO  Client: Verifying our application has not requested more than the maximum memory capability of the cluster (24271 MB per container)\n",
      "2021-02-13 13:51:17,007 INFO  Client: Will allocate AM container, with 2432 MB memory including 384 MB overhead\n",
      "2021-02-13 13:51:17,008 INFO  Client: Setting up container launch context for our AM\n",
      "2021-02-13 13:51:17,013 INFO  Client: Setting up the launch environment for our AM container\n",
      "2021-02-13 13:51:17,023 INFO  Client: Preparing resources for our AM container\n",
      "2021-02-13 13:51:17,786 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/metrics.properties\n",
      "2021-02-13 13:51:17,863 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/log4j.properties\n",
      "2021-02-13 13:51:17,869 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/hive-site.xml\n",
      "2021-02-13 13:51:18,016 INFO  Client: Uploading resource file:/tmp/spark-4a519839-362d-435d-a164-260306d4d5cb/__spark_conf__5467427819598746872.zip -> hdfs:/Projects/blah/Resources/.sparkStaging/application_1613224209876_0002/__spark_conf__.zip\n",
      "2021-02-13 13:51:18,478 INFO  SecurityManager: Changing view acls to: livy,blah__meb10179\n",
      "2021-02-13 13:51:18,478 INFO  SecurityManager: Changing modify acls to: livy,blah__meb10179\n",
      "2021-02-13 13:51:18,479 INFO  SecurityManager: Changing view acls groups to: \n",
      "2021-02-13 13:51:18,479 INFO  SecurityManager: Changing modify acls groups to: \n",
      "2021-02-13 13:51:18,480 INFO  SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(livy, blah__meb10179); groups with view permissions: Set(); users  with modify permissions: Set(livy, blah__meb10179); groups with modify permissions: Set()\n",
      "2021-02-13 13:51:18,538 INFO  EsServiceCredentialProvider: Loaded EsServiceCredentialProvider\n",
      "2021-02-13 13:51:19,471 INFO  EsServiceCredentialProvider: Hadoop Security Enabled = [false]\n",
      "2021-02-13 13:51:19,471 INFO  EsServiceCredentialProvider: ES Auth Method = [SIMPLE]\n",
      "2021-02-13 13:51:19,471 INFO  EsServiceCredentialProvider: Are creds required = [false]\n",
      "2021-02-13 13:51:19,478 INFO  Client: Submitting application application_1613224209876_0002 to ResourceManager\n",
      "2021-02-13 13:51:19,542 INFO  YarnClientImpl: Submitted application application_1613224209876_0002\n",
      "2021-02-13 13:51:19,547 INFO  Client: Application report for application_1613224209876_0002 (state: GENERATING_SECURITY_MATERIAL)\n",
      "2021-02-13 13:51:19,551 INFO  Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1613224279498\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://resourcemanager.service.consul:8088/proxy/application_1613224209876_0002/\n",
      "\t user: blah__meb10179\n",
      "2021-02-13 13:51:19,556 INFO  ShutdownHookManager: Shutdown hook called\n",
      "2021-02-13 13:51:19,557 INFO  ShutdownHookManager: Deleting directory /tmp/spark-4a519839-362d-435d-a164-260306d4d5cb\n",
      "2021-02-13 13:51:19,570 INFO  ShutdownHookManager: Deleting directory /tmp/spark-910cc854-850d-484f-933a-a7065a8eff15\n",
      "\n",
      "stderr: \n",
      "\n",
      "YARN Diagnostics: \n",
      "Invalid host name: local host is: (unknown); destination host is: \"resourcemanager.service.consul\":8032; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost.\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "party = spark.read\\\n",
    "             .option(\"inferSchema\", \"true\")\\\n",
    "             .option(\"header\", \"true\")\\\n",
    "             .format(\"csv\")\\\n",
    "             .load(\"hdfs:///Projects/{}/Resources/party.csv\".format(hdfs.project_name()))\n",
    "party.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tSession 8 unexpectedly reached final status 'dead'. See logs:\n",
      "stdout: \n",
      "2021-02-13 13:51:15,839 WARN  NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2021-02-13 13:51:15,960 WARN  DependencyUtils: Local jar /srv/hops/spark/jars/datanucleus-api.jar does not exist, skipping.\n",
      "2021-02-13 13:51:16,101 INFO  RMProxy: Connecting to ResourceManager at resourcemanager.service.consul/10.0.0.4:8032\n",
      "2021-02-13 13:51:16,890 INFO  Client: Requesting a new application from cluster with 1 NodeManagers\n",
      "2021-02-13 13:51:16,981 INFO  Configuration: found resource resource-types.xml at file:/srv/hops/hadoop-3.2.0.1-EE-RC0/etc/hadoop/resource-types.xml\n",
      "2021-02-13 13:51:16,990 INFO  ResourceUtils: Adding resource type - name = yarn.io/gpu, units = , type = COUNTABLE\n",
      "2021-02-13 13:51:17,007 INFO  Client: Verifying our application has not requested more than the maximum memory capability of the cluster (24271 MB per container)\n",
      "2021-02-13 13:51:17,007 INFO  Client: Will allocate AM container, with 2432 MB memory including 384 MB overhead\n",
      "2021-02-13 13:51:17,008 INFO  Client: Setting up container launch context for our AM\n",
      "2021-02-13 13:51:17,013 INFO  Client: Setting up the launch environment for our AM container\n",
      "2021-02-13 13:51:17,023 INFO  Client: Preparing resources for our AM container\n",
      "2021-02-13 13:51:17,786 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/metrics.properties\n",
      "2021-02-13 13:51:17,863 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/log4j.properties\n",
      "2021-02-13 13:51:17,869 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/hive-site.xml\n",
      "2021-02-13 13:51:18,016 INFO  Client: Uploading resource file:/tmp/spark-4a519839-362d-435d-a164-260306d4d5cb/__spark_conf__5467427819598746872.zip -> hdfs:/Projects/blah/Resources/.sparkStaging/application_1613224209876_0002/__spark_conf__.zip\n",
      "2021-02-13 13:51:18,478 INFO  SecurityManager: Changing view acls to: livy,blah__meb10179\n",
      "2021-02-13 13:51:18,478 INFO  SecurityManager: Changing modify acls to: livy,blah__meb10179\n",
      "2021-02-13 13:51:18,479 INFO  SecurityManager: Changing view acls groups to: \n",
      "2021-02-13 13:51:18,479 INFO  SecurityManager: Changing modify acls groups to: \n",
      "2021-02-13 13:51:18,480 INFO  SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(livy, blah__meb10179); groups with view permissions: Set(); users  with modify permissions: Set(livy, blah__meb10179); groups with modify permissions: Set()\n",
      "2021-02-13 13:51:18,538 INFO  EsServiceCredentialProvider: Loaded EsServiceCredentialProvider\n",
      "2021-02-13 13:51:19,471 INFO  EsServiceCredentialProvider: Hadoop Security Enabled = [false]\n",
      "2021-02-13 13:51:19,471 INFO  EsServiceCredentialProvider: ES Auth Method = [SIMPLE]\n",
      "2021-02-13 13:51:19,471 INFO  EsServiceCredentialProvider: Are creds required = [false]\n",
      "2021-02-13 13:51:19,478 INFO  Client: Submitting application application_1613224209876_0002 to ResourceManager\n",
      "2021-02-13 13:51:19,542 INFO  YarnClientImpl: Submitted application application_1613224209876_0002\n",
      "2021-02-13 13:51:19,547 INFO  Client: Application report for application_1613224209876_0002 (state: GENERATING_SECURITY_MATERIAL)\n",
      "2021-02-13 13:51:19,551 INFO  Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1613224279498\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://resourcemanager.service.consul:8088/proxy/application_1613224209876_0002/\n",
      "\t user: blah__meb10179\n",
      "2021-02-13 13:51:19,556 INFO  ShutdownHookManager: Shutdown hook called\n",
      "2021-02-13 13:51:19,557 INFO  ShutdownHookManager: Deleting directory /tmp/spark-4a519839-362d-435d-a164-260306d4d5cb\n",
      "2021-02-13 13:51:19,570 INFO  ShutdownHookManager: Deleting directory /tmp/spark-910cc854-850d-484f-933a-a7065a8eff15\n",
      "\n",
      "stderr: \n",
      "\n",
      "YARN Diagnostics: \n",
      "Invalid host name: local host is: (unknown); destination host is: \"resourcemanager.service.consul\":8032; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost.\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "party=party.withColumn('partyType', party_2_code_udf(F.col('partyType'))).toDF(\"id\",\"type\")\n",
    "party.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tSession 8 unexpectedly reached final status 'dead'. See logs:\n",
      "stdout: \n",
      "2021-02-13 13:51:15,839 WARN  NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2021-02-13 13:51:15,960 WARN  DependencyUtils: Local jar /srv/hops/spark/jars/datanucleus-api.jar does not exist, skipping.\n",
      "2021-02-13 13:51:16,101 INFO  RMProxy: Connecting to ResourceManager at resourcemanager.service.consul/10.0.0.4:8032\n",
      "2021-02-13 13:51:16,890 INFO  Client: Requesting a new application from cluster with 1 NodeManagers\n",
      "2021-02-13 13:51:16,981 INFO  Configuration: found resource resource-types.xml at file:/srv/hops/hadoop-3.2.0.1-EE-RC0/etc/hadoop/resource-types.xml\n",
      "2021-02-13 13:51:16,990 INFO  ResourceUtils: Adding resource type - name = yarn.io/gpu, units = , type = COUNTABLE\n",
      "2021-02-13 13:51:17,007 INFO  Client: Verifying our application has not requested more than the maximum memory capability of the cluster (24271 MB per container)\n",
      "2021-02-13 13:51:17,007 INFO  Client: Will allocate AM container, with 2432 MB memory including 384 MB overhead\n",
      "2021-02-13 13:51:17,008 INFO  Client: Setting up container launch context for our AM\n",
      "2021-02-13 13:51:17,013 INFO  Client: Setting up the launch environment for our AM container\n",
      "2021-02-13 13:51:17,023 INFO  Client: Preparing resources for our AM container\n",
      "2021-02-13 13:51:17,786 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/metrics.properties\n",
      "2021-02-13 13:51:17,863 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/log4j.properties\n",
      "2021-02-13 13:51:17,869 INFO  Client: Source and destination file systems are the same. Not copying hdfs:/user/spark/hive-site.xml\n",
      "2021-02-13 13:51:18,016 INFO  Client: Uploading resource file:/tmp/spark-4a519839-362d-435d-a164-260306d4d5cb/__spark_conf__5467427819598746872.zip -> hdfs:/Projects/blah/Resources/.sparkStaging/application_1613224209876_0002/__spark_conf__.zip\n",
      "2021-02-13 13:51:18,478 INFO  SecurityManager: Changing view acls to: livy,blah__meb10179\n",
      "2021-02-13 13:51:18,478 INFO  SecurityManager: Changing modify acls to: livy,blah__meb10179\n",
      "2021-02-13 13:51:18,479 INFO  SecurityManager: Changing view acls groups to: \n",
      "2021-02-13 13:51:18,479 INFO  SecurityManager: Changing modify acls groups to: \n",
      "2021-02-13 13:51:18,480 INFO  SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(livy, blah__meb10179); groups with view permissions: Set(); users  with modify permissions: Set(livy, blah__meb10179); groups with modify permissions: Set()\n",
      "2021-02-13 13:51:18,538 INFO  EsServiceCredentialProvider: Loaded EsServiceCredentialProvider\n",
      "2021-02-13 13:51:19,471 INFO  EsServiceCredentialProvider: Hadoop Security Enabled = [false]\n",
      "2021-02-13 13:51:19,471 INFO  EsServiceCredentialProvider: ES Auth Method = [SIMPLE]\n",
      "2021-02-13 13:51:19,471 INFO  EsServiceCredentialProvider: Are creds required = [false]\n",
      "2021-02-13 13:51:19,478 INFO  Client: Submitting application application_1613224209876_0002 to ResourceManager\n",
      "2021-02-13 13:51:19,542 INFO  YarnClientImpl: Submitted application application_1613224209876_0002\n",
      "2021-02-13 13:51:19,547 INFO  Client: Application report for application_1613224209876_0002 (state: GENERATING_SECURITY_MATERIAL)\n",
      "2021-02-13 13:51:19,551 INFO  Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1613224279498\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://resourcemanager.service.consul:8088/proxy/application_1613224209876_0002/\n",
      "\t user: blah__meb10179\n",
      "2021-02-13 13:51:19,556 INFO  ShutdownHookManager: Shutdown hook called\n",
      "2021-02-13 13:51:19,557 INFO  ShutdownHookManager: Deleting directory /tmp/spark-4a519839-362d-435d-a164-260306d4d5cb\n",
      "2021-02-13 13:51:19,570 INFO  ShutdownHookManager: Deleting directory /tmp/spark-910cc854-850d-484f-933a-a7065a8eff15\n",
      "\n",
      "stderr: \n",
      "\n",
      "YARN Diagnostics: \n",
      "Invalid host name: local host is: (unknown); destination host is: \"resourcemanager.service.consul\":8032; java.net.UnknownHostException; For more details see:  http://wiki.apache.org/hadoop/UnknownHost.\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "party_fg = fs.create_feature_group(name=\"party_fg\",\n",
    "                                       version=1,\n",
    "                                       primary_key=[\"partyId\"],\n",
    "                                       description=\"party fg\",\n",
    "                                       time_travel_format=None,                                        \n",
    "                                       statistics_config=False)\n",
    "party_fg.save(party)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
