{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th></tr><tr><td>18</td><td>application_1612044880670_0019</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://davitamlgpu-master.internal.cloudapp.net:8088/proxy/application_1612044880670_0019/\">Link</a></td><td><a target=\"_blank\" href=\"http://davitamlgpu-worker-1.internal.cloudapp.net:8042/node/containerlogs/container_e04_1612044880670_0019_01_000001/devaml__meb10179\">Link</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "<pyspark.sql.session.SparkSession object at 0x7f3a16341b90>"
     ]
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hsfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully."
     ]
    }
   ],
   "source": [
    "# Create a connection\n",
    "connection = hsfs.connection()\n",
    "# Get the feature store handle for the project's feature store\n",
    "fs = connection.get_feature_store(\"amlsim_featurestore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|target|           embedding|\n",
      "+------+--------------------+\n",
      "|     1|[0.48607898, -0.1...|\n",
      "|     1|[-0.47727537, 0.1...|\n",
      "|     1|[0.22666311, -0.5...|\n",
      "|     1|[-0.7683222, 0.13...|\n",
      "|     1|[0.39057684, 0.96...|\n",
      "|     1|[-0.7735877, 0.05...|\n",
      "|     1|[-0.095639706, 0....|\n",
      "|     1|[0.47294903, -0.3...|\n",
      "|     1|[-0.39901948, -0....|\n",
      "|     1|[0.7641628, 0.305...|\n",
      "|     1|[0.69657683, 0.11...|\n",
      "|     1|[-0.0423429, -0.6...|\n",
      "|     1|[0.868845, 0.0528...|\n",
      "|     1|[-0.43252254, 0.5...|\n",
      "|     1|[0.06834984, -0.7...|\n",
      "|     1|[-0.47181463, -0....|\n",
      "|     1|[-0.20204735, -0....|\n",
      "|     1|[0.80187464, 0.07...|\n",
      "|     1|[0.4038105, 0.544...|\n",
      "|     1|[0.9727471, 0.077...|\n",
      "+------+--------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "eval_df = fs.get_training_dataset(\"gan_eval_df\", 1).read()\n",
    "train_df = fs.get_training_dataset(\"gan_non_sar_training_df\", 1).read() \n",
    "emb_df = eval_df.union(train_df) #.where(eval_df.target==1)\n",
    "emb_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|target|           embedding|\n",
      "+------+--------------------+\n",
      "|     0|[-0.7185197, -0.5...|\n",
      "|     0|[-0.5465119, 0.73...|\n",
      "|     0|[-0.2560954, 0.67...|\n",
      "|     0|[0.03640437, 0.76...|\n",
      "|     0|[-0.9657593, -0.7...|\n",
      "|     0|[-0.6548383, -0.3...|\n",
      "|     0|[0.18099833, 0.23...|\n",
      "|     0|[-7.095337E-4, -0...|\n",
      "|     0|[0.087100744, -0....|\n",
      "|     0|[0.5484538, -0.84...|\n",
      "|     0|[-0.678113, -0.36...|\n",
      "|     0|[-0.29078293, -0....|\n",
      "|     0|[0.12922144, 0.77...|\n",
      "|     0|[-0.5990057, -0.0...|\n",
      "|     0|[-0.31415105, 0.5...|\n",
      "|     0|[-0.2342267, -0.8...|\n",
      "|     0|[-0.43512964, -0....|\n",
      "|     0|[-0.2187376, -0.3...|\n",
      "|     0|[0.10286474, -0.4...|\n",
      "|     0|[-0.89329696, 0.2...|\n",
      "+------+--------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "emb_df.where(emb_df.target == 0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get fg as pandas\n",
    "from pyspark.sql.functions import explode\n",
    "dlist = emb_df.columns\n",
    "emb_pdf = emb_df.select(dlist+[(emb_df.embedding[x]).alias(\"Value\"+str(x+1)) for x in range(0, 128)]).drop(\"embedding\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%spark -o emb_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of       target    Value1    Value2  ...  Value126  Value127  Value128\n",
      "0          1  0.486079 -0.189078  ... -0.837726  0.279952 -0.123464\n",
      "1          1 -0.477275  0.147429  ...  0.718477  0.100887  0.402870\n",
      "2          1  0.226663 -0.533207  ... -0.564404 -0.305063 -0.916829\n",
      "3          1 -0.768322  0.132927  ...  0.296606  0.620593 -0.176689\n",
      "4          1  0.390577  0.961654  ... -0.193944  0.149571  0.569036\n",
      "...      ...       ...       ...  ...       ...       ...       ...\n",
      "7342       0  0.699275 -0.162821  ...  0.535009 -0.311959  0.359504\n",
      "7343       0  0.726155  0.538397  ... -0.363945 -0.011806 -0.075307\n",
      "7344       0  0.816660  0.897723  ... -0.815421 -0.554857 -0.909673\n",
      "7345       0  0.936104  0.153948  ... -0.073009  0.574821 -0.092643\n",
      "7346       0  0.963763 -0.816191  ...  0.546268 -0.630459 -0.368822\n",
      "\n",
      "[7347 rows x 129 columns]>"
     ]
    }
   ],
   "source": [
    "emb_pdf.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Unexpected character ('#' (code 35)): expected a valid value (number, String, array, object, 'true', 'false' or 'null')\n",
      " at [Source: #; line: 1, column: 2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#%%local \n",
    "\n",
    "import os\n",
    "import sys\n",
    "import uuid\n",
    "import random    \n",
    "    \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pydoop.hdfs as pydoop\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "    \n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from hops import hdfs\n",
    "\n",
    "\n",
    "walk_number = 40 \n",
    "walk_length = 5 \n",
    "emb_size = 128\n",
    "\n",
    "node_embeddings = emb_pdf.loc[:, emb_pdf.columns != 'target']\n",
    "target = emb_pdf.target\n",
    "embeddings_features = pd.DataFrame(node_embeddings, index=emb_pdf.target.values)\n",
    "print(len(embeddings_features.index.values))\n",
    "\n",
    "def _tsne_plot(node_embeddings, walk_number, walk_length, emb_size):\n",
    "        transform = TSNE  # PCA\n",
    "        trans = transform(n_components=2, random_state=123)        \n",
    "        #trans = transform(n_components=2, verbose=1, perplexity=40, n_iter=1500)\n",
    "        \n",
    "        node_embeddings_2d = trans.fit_transform(node_embeddings)\n",
    "\n",
    "\n",
    "        # draw the embedding points, coloring them by the target label (paper subject)\n",
    "        alpha = 0.7\n",
    "\n",
    "        \"\"\"\n",
    "        ###############\n",
    "        labels = pd.read_csv(pydoop.path.abspath(hdfs.get_plain_path([path for path in hdfs.ls(labels_file) if path.endswith(\"csv\")][0])))\n",
    "        label_map = pd.Series(labels.is_sar.values,index=labels.id).to_dict()\n",
    "        node_colours = []\n",
    "        for target in node_embeddings.index.values:\n",
    "            if target in label_map:\n",
    "                node_colours.append(label_map[target])\n",
    "            else:\n",
    "                node_colours.append(0)\n",
    "        ################        \n",
    "        \"\"\"\n",
    "        node_colours = node_embeddings.index.values\n",
    "\n",
    "        plt.figure(figsize=(300, 300))\n",
    "        plt.axes().set(aspect=\"equal\")\n",
    "        plt.scatter(\n",
    "            node_embeddings_2d[:, 0],\n",
    "            node_embeddings_2d[:, 1],\n",
    "            c=node_colours,\n",
    "            cmap=\"jet\",\n",
    "            alpha=alpha,\n",
    "        )\n",
    "        #plt.title(\"{} visualization of node embeddings\".format(transform.__name__))\n",
    "        tsne_file = 'node_embeddings_features_%d_%d_%d.tsne.pdf' % (walk_number, walk_length, emb_size)\n",
    "        plt.savefig(tsne_file)\n",
    "        plt.close()\n",
    "        hdfs.copy_to_hdfs(tsne_file, \"Resources\", overwrite=True, project=\"devaml\")\n",
    "        #plt.show()\n",
    "    \n",
    "_tsne_plot(embeddings_features, walk_number, walk_length, emb_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}